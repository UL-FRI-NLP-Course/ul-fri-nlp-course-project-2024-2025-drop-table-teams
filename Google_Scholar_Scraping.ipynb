{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5efcdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly\n",
    "import requests\n",
    "import os\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26565f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "347e5192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scholarly.publication_parser._SearchScholarIterator at 0x7f30c176f310>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = scholarly.search_pubs('Slovenia Estonia differences')\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ec8476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male-female differences in labor market outcomes during the early transition to market: The cases of Estonia and Slovenia\n",
      "https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=620581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'papers.ssrn.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eprint URL is not a direct link to a PDF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'papers.ssrn.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = next(search)\n",
    "title = result[\"bib\"][\"title\"]\n",
    "print(title)\n",
    "#print(result[\"pub_url\"])\n",
    "if \"eprint_url\" in result:\n",
    "    print(result[\"eprint_url\"])\n",
    "    response = requests.get(result[\"eprint_url\"], stream=True, verify=False)\n",
    "\n",
    "    content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "    is_pdf = \"application/pdf\" in content_type or result[\"eprint_url\"].lower().endswith(\".pdf\")\n",
    "\n",
    "    if response.status_code == 200 and is_pdf:\n",
    "        # Determine a safe filename\n",
    "        title = title.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        filename = os.path.join(data_dir, f\"{title}.pdf\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"PDF saved as {filename}\")\n",
    "    else:\n",
    "        print(\"The eprint URL is not a direct link to a PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6889bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'container_type': 'Publication',\n",
       " 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>,\n",
       " 'bib': {'title': 'Crisis and opportunity: Varieties of capitalism and varieties of crisis responses in Estonia and Slovenia',\n",
       "  'author': ['M Feldmann'],\n",
       "  'pub_year': '2017',\n",
       "  'venue': 'European Journal of Industrial Relations',\n",
       "  'abstract': 'institutions have displayed greater stability than those in Slovenia.  selection of Estonia  and Slovenia in more detail. After that, I analyse the crisis experience in Estonia and Slovenia;'},\n",
       " 'filled': False,\n",
       " 'gsrank': 5,\n",
       " 'pub_url': 'https://journals.sagepub.com/doi/abs/10.1177/0959680116672280',\n",
       " 'author_id': ['IdHSCSgAAAAJ'],\n",
       " 'url_scholarbib': '/scholar?hl=en&q=info:O1Q7hmsWgDAJ:scholar.google.com/&output=cite&scirp=4&hl=en',\n",
       " 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlovenia%2BEstonia%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O1Q7hmsWgDAJ&ei=QKIbaM_KCO2h6rQPp8GU8As&json=',\n",
       " 'num_citations': 42,\n",
       " 'citedby_url': '/scholar?cites=3494817961908851771&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       " 'url_related_articles': '/scholar?q=related:O1Q7hmsWgDAJ:scholar.google.com/&scioq=Slovenia+Estonia&hl=en&as_sdt=0,33',\n",
       " 'eprint_url': 'https://research-information.bris.ac.uk/files/65457095/EJIR_accepted_2016.pdf'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48399b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Distractor-Aware Memory for Visual Object Tracking with SAM2\n",
      "Jovana Videnovic∗, Alan Lukezic∗, Matej Kristan\n",
      "Faculty of Computer and Information Science, University of Ljubljana, Slovenia\n",
      "jv8043@student.uni-lj.si, {alan.lukezic, matej.kristan}@fri.uni-lj.si\n",
      "Abstract\n",
      "Memory-based trackers are video object segmentation\n",
      "methods that form the target model by concatenating re-\n",
      "cently tracked frames into a memory buffer and localize\n",
      "the target by attending the current image to the buffered\n",
      "frames. While already achieving top performance on many\n",
      "benchmarks, it was the recent release of SAM2 that placed\n",
      "memory-based trackers into focus of the visual object track-\n",
      "ing community. Nevertheless, modern trackers still strug-\n",
      "gle in the presence of distractors. We argue that a more\n",
      "sophisticated memory model is required, and propose a\n",
      "new distractor-aware memory model for SAM2 and an\n",
      "introspection-based update strategy that jointly addresses\n",
      "the segmentation accuracy as well as tracking robustness.\n",
      "The resulting tracker is denoted as SAM2.1++. We also\n",
      "propose a new distractor-distilled DiDi dataset to study the\n",
      "distractor problem better. SAM2.1++ outperforms SAM2.1\n",
      "and related SAM memory extensions on seven benchmarks\n",
      "and sets a solid new state-of-the-art on six of them. The\n",
      "code and the new dataset will be available on https:\n",
      "//github.com/jovanavidenovic/DAM4SAM.\n",
      "1. Introduction\n",
      "General visual object tracking is a classical computer vision\n",
      "problem that considers the localization of an arbitrary target\n",
      "in the video, given a single supervised training example in\n",
      "the first frame. The major source of tracking failures are so-\n",
      "called distractors, i.e., image regions that are difficult to dis-\n",
      "tinguish from the tracked object, given the available target\n",
      "model (see Figure 1). These can be nearby objects similar\n",
      "to the tracked target (external distractors) or similar regions\n",
      "on the object when tracking only a part of the object (inter-\n",
      "nal distractors). When the target leaves and re-enters the\n",
      "field of view, the external distractors become particularly\n",
      "challenging.\n",
      "Various approaches have been proposed to reduce the vi-\n",
      "∗The authors contributed equally.\n",
      "0.673\n",
      "0.692\n",
      "0.753\n",
      "MS_AOT\n",
      "SAM2.1\n",
      "SAM2.1++\n",
      "VOT2022: EAO\n",
      "+9%\n",
      "+3%\n",
      "+12%\n",
      "Predicted mask \n",
      "AlternaƟve mask\n",
      "VOT2022\n",
      "winner\n",
      "Target\n",
      "t\n",
      "t+T\n",
      "Distractor detected!\n",
      "Target\n",
      "SAM2.1      and SAM2.1++     : Performance in presence of distractors\n",
      "SAM2 output introspecƟon \n",
      "Figure 1. SAM2.1++ distractor-aware memory (DAM) update is\n",
      "triggered by the divergence between the predicted and the alterna-\n",
      "tive masks (top left). This resolves the visual ambiguity and in-\n",
      "creases tracking robustness (bottom). DAM leads to a significant\n",
      "performance boost, setting a new sota on VOT2022 (top-right).\n",
      "sual ambiguity caused by distractors. These include learn-\n",
      "ing discriminative features [2, 5, 6, 10, 43] or explicitly\n",
      "modeling the foreground-background by dedicated mod-\n",
      "ules [9, 28, 29, 50]. An emerging paradigm, already po-\n",
      "sitioned at the top of the major benchmarks [23–25], are\n",
      "memory-based frameworks, which localize the target by\n",
      "pixel association with the past tracked frames [8, 47, 51].\n",
      "The memory-based methods construct the target model\n",
      "by concatenating sequences of entire images with the seg-\n",
      "mented target, thus implicitly encoding the present dis-\n",
      "tractors.\n",
      "[51] argue that the visual redundancy in the\n",
      "large memory leads to reduced localization capability due\n",
      "to the nature of cross-attention.\n",
      "They show that limit-\n",
      "ing the memory to the most recent frames and tempo-\n",
      "rally time-stamping them in fact improves tracking. This\n",
      "paradigm was further verified by the recent tracking foun-\n",
      "1\n",
      "arXiv:2411.17576v2  [cs.CV]  4 Dec 2024\n",
      "\n",
      "dation model SAM2 [36], which sets a solid state-of-the-\n",
      "art across several video segmentation and tracking bench-\n",
      "marks [13, 18, 25, 35, 41].\n",
      "We argue that while the recent target appearances in the\n",
      "memory are required for accurate segmentation, another\n",
      "type of memory is required to distinguish the target from\n",
      "challenging distractors. To support this claim, we propose\n",
      "a new distractor-aware memory (DAM) and update mecha-\n",
      "nism for SAM2. The new memory is divided by its tracking\n",
      "functionality into two parts: the recent appearances mem-\n",
      "ory (RAM) and distractor-resolving memory (DRM). While\n",
      "RAM contains the recent target appearances sampled at reg-\n",
      "ular intervals, DRM contains anchor frames that help dis-\n",
      "criminate the target from critical distractors. A novel DRM\n",
      "updating mechanism is proposed that exploits the informa-\n",
      "tion of SAM2 output, which has been ignored so far by the\n",
      "tracking research.\n",
      "In addition, we observe that standard benchmarks con-\n",
      "tain many sequences, which are no longer considered chal-\n",
      "lenging by modern standards. The high performance on\n",
      "these sequences overwhelms the total score, saturates the\n",
      "benchmarks, and does not properly expose the tracking ad-\n",
      "vances. To address this, we semi-automatically distill sev-\n",
      "eral benchmarks into a distractor-distilled tracking dataset\n",
      "(DiDi).\n",
      "In summary, our main contribution is the new distractor-\n",
      "aware memory DAM for SAM2 and the updating strategy,\n",
      "resulting in SAM2++. To the best of our knowledge, this\n",
      "is the first memory formulation that divides and updates\n",
      "the memory with respect to its function in tracking. Our\n",
      "secondary contribution is the new DiDi dataset that more\n",
      "clearly exposes tracking advances in the presence of dis-\n",
      "tractors. With no additional training, SAM2.1++ substan-\n",
      "tially outperforms SAM2.1 in robustness on several stan-\n",
      "dard bounding box and segmentation tracking benchmarks,\n",
      "including the new DiDi dataset, and sets a new state-of-the-\n",
      "art in visual object tracking.\n",
      "2. Related Work\n",
      "Transformers are currently the dominant methodology in vi-\n",
      "sual object tracking [23] and can be largely categorized into\n",
      "classification- and regression-based [5], corner prediction-\n",
      "based [43], and sequence-learning-based trackers [6].\n",
      "The recent top-performing tracking frameworks are in-\n",
      "spired by the video object segmentation methods based on\n",
      "memory networks [8, 9, 36, 47, 51]. These methods em-\n",
      "bed predictions from past frames into memory, therefore ex-\n",
      "tending contextual information beyond just the initial or the\n",
      "previous frame. The attention mechanism is typically used\n",
      "to link frame representations stored in the memory with fea-\n",
      "tures extracted from the current frame. In the initial meth-\n",
      "ods like [47], the arriving frames were continually added to\n",
      "the memory. This led to theoretically unbounded increase\n",
      "in computational complexity and GPU memory.\n",
      "This issue was addressed in [8, 9] by using multiple\n",
      "memory storages and efficient compression schemes to cap-\n",
      "ture different temporal contexts, enhancing performance on\n",
      "long-term videos. Alternatively, [51] proposed to restrict\n",
      "the memory to the most recent frames with temporal stamp-\n",
      "ing, which led to improved localization. The principle of the\n",
      "restricted memory is followed by the SAM2 [36] foundation\n",
      "model, which stores last 6 frames and the initial frame in the\n",
      "memory. Recently, SAM2Long [14] proposed a training-\n",
      "free method to enhance the performance of SAM2 [36] on\n",
      "long-term sequences by determining the optimal trajectory\n",
      "from multiple segmentation pathways using a constrained\n",
      "tree search.\n",
      "Most of the existing tracking methods do not explicitly\n",
      "address tracking in the presence of distractors, even though\n",
      "distractors are a major source of tracking failures.\n",
      "Dis-\n",
      "criminative (deep) correlation filters [12, 33] are theoret-\n",
      "ically suitable to handle distractors but are in practice out-\n",
      "performed by the modern transformer-based trackers. How-\n",
      "ever, there have been some recent attempts to address dis-\n",
      "tractors. KeepTrack [32] casts the problem as a multi-target\n",
      "tracking setup, where it identifies target candidates and po-\n",
      "tential distractors, which are then associated with previ-\n",
      "ously propagated identities using a learned association net-\n",
      "work. However, the method relies on accurate detection and\n",
      "cannot address internal distractors in practice. In [29] tar-\n",
      "get localization accuracy and robustness are treated as two\n",
      "distinct tasks, which is demonstrated to be beneficial in sit-\n",
      "uations with distractors.\n",
      "Despite explicit distractor han-\n",
      "dling mechanism, these methods lead to complicated ar-\n",
      "chitectures and cannot fully exploit the learning potential\n",
      "of modern frameworks. In contrast, memory-based meth-\n",
      "ods [8, 9, 36, 51] have the capacity to implicitly handle dis-\n",
      "tractors in an elegant way, since they store entire images\n",
      "and apply a learnable localization by segmentation. How-\n",
      "ever, the existing memory management methods are not de-\n",
      "signed to effectively handle the distractors.\n",
      "3. Distractor-aware memory for SAM2\n",
      "This section describes the new DAM memory model for\n",
      "SAM2. Section 3.1 briefly outlines the SAM2 architecture,\n",
      "while the new model is described in Section 3.2.\n",
      "3.1. SAM2 preliminaries\n",
      "SAM2 extends the Segment Anything Model (SAM) [20],\n",
      "originally developed for interactive class-agnostic image\n",
      "segmentation, to video segmentation. It consists of four\n",
      "main components: (i) image encoder, (ii) prompt encoder,\n",
      "(iii) memory bank, and (iv) mask decoder.\n",
      "The image encoder applies ViT Hiera1 backbone [37]\n",
      "1ViT-L version is used in all our experiments.\n",
      "2\n",
      "\n",
      "to embed the input image. Interactive inputs (e.g., posi-\n",
      "tive/negative clicks) are absorbed by the prompt encoder\n",
      "and used for output mask refinement, however, note that\n",
      "these are not applicable in the general object tracking setup.\n",
      "The memory bank consists of the encoded initialization\n",
      "frame with a user-provided segmentation mask and six\n",
      "recent frames with segmentation masks generated by the\n",
      "tracking output. Temporal encodings are applied to the six\n",
      "recent frames to encode the frame order, while such encod-\n",
      "ing is not applied to the initialization frame to indicate its\n",
      "unique property of being a single supervised training exam-\n",
      "ple and thus serves as a sort of target prior model.\n",
      "The memory bank transfers pixel-wise labels onto the\n",
      "current image by attending the features in the current frame\n",
      "to all memory frames, producing memory-conditioned fea-\n",
      "tures. The features are then decoded by the mask decoder,\n",
      "which predicts three output masks along with the IoU pre-\n",
      "diction for each. The mask with the highest IoU is chosen\n",
      "as the tracking output.\n",
      "SAM2 applies a variant of the memory management pro-\n",
      "posed in [51]. The initialization frame is always kept in the\n",
      "memory, while the the six recent frames are updated at ev-\n",
      "ery new frame by a first-in-first-out protocol. The memory\n",
      "and the management mechanism are visualized in Figure 2,\n",
      "while the reader is referred to [36] for other details.\n",
      "3.2. Distractor-aware memory – DAM\n",
      "Related works [8, 36, 47, 51] have clearly demonstrated the\n",
      "importance of the most recent frames, which are required to\n",
      "address target appearance changes and ensure accurate seg-\n",
      "mentation. However, a different type of frames is required\n",
      "to prevent drifting in the presence of critical distractors and\n",
      "for reliable target re-detection.\n",
      "We propose to compose the memory with respect to its\n",
      "function during tracking into (i) recent appearance mem-\n",
      "ory (RAM) and (ii) distractor resolving memory (DRM).\n",
      "RAM and DRM together form the distractor-aware mem-\n",
      "ory (DAM), visualized in Figure 2. The function of RAM\n",
      "is to ensure segmentation accuracy in the considered frame,\n",
      "thus we design it akin to the current SAM2 [36] memory. It\n",
      "is composed of a FIFO buffer with 1\n",
      "2NDAM = 3 slots, con-\n",
      "tains the most recent target appearances, and applies tem-\n",
      "poral encoding to identify temporally more relevant frames\n",
      "for the task.\n",
      "On the other hand, DRM serves for ensuring tracking\n",
      "robustness and re-detection. It should contain accurately\n",
      "segmented frames with critical recent distractors, including\n",
      "the initialization frame. It is thus composed of a slot re-\n",
      "served for the initialization frame and a FIFO buffer with\n",
      "1\n",
      "2NDAM = 3 anchor frames updated during tracking2.\n",
      "Since the purpose of DRM is to encode critical information\n",
      "2At tracker initialization, RAM occupies all NDAM slots and drops to\n",
      "1\n",
      "2 NDAM as DRM entries arrive, to fully exploit the available capacity.\n",
      "for resolving distractors, it does not apply temporal encod-\n",
      "ing. Note that the pre-trained SAM2 already contains the\n",
      "building blocks to implement the proposed memory struc-\n",
      "ture.\n",
      "3.2.1. RAM management protocol\n",
      "A crucial element of the memory-based methods is the\n",
      "memory management protocol. To efficiently exploit the\n",
      "available memory slots, the memory should not be updated\n",
      "at every frame, since the consecutive frames are highly cor-\n",
      "related. In fact, [51] argue that visual redundancy in mem-\n",
      "ory should be avoided in attention-based localization. RAM\n",
      "is thus updated every ∆= 5 frames and includes the most\n",
      "recent frame since it is the most relevant for accurate target\n",
      "segmentation in the considered frame.\n",
      "SAM2 [36] updates the memory at every frame, includ-\n",
      "ing when the target is absent. However, even for a very short\n",
      "occlusion, the memory will quickly fill up with frames with-\n",
      "out the target, which reduces the target appearance diversity\n",
      "in the model, leading to a reduced segmentation accuracy\n",
      "upon target re-appearance. Furthermore, failing to re-detect\n",
      "the target leads to incorrectly updating the memory by an\n",
      "empty mask, which may cause error accumulation and ulti-\n",
      "mately re-detection failure. We thus propose to not update\n",
      "RAM when the target is not present, i.e., when the predicted\n",
      "target mask is empty.\n",
      "3.2.2. DRM management protocol\n",
      "DRM inherits the initial update rules from RAM, i.e., up-\n",
      "date only when the target is present and at least ∆= 5\n",
      "frames have passed since the last update. It considers an\n",
      "additional rule to identify anchor frames containing criti-\n",
      "cal distractors. In particular, drifting to a distractor may be\n",
      "avoided by including a past temporally nearby frame with\n",
      "this distractor accurately segmented as the background. Re-\n",
      "call that SAM2 predicts three output masks and selects the\n",
      "one with the highest predicted IoU (Section 3.1), which\n",
      "means we can consider it as a multi-hypothesis prediction\n",
      "model. Our preliminary study showed that in the frames\n",
      "before the failure occurs, SAM2 in fact detects such dis-\n",
      "tractors in the alternative two predicted output masks (see\n",
      "Figure 1). We thus propose a simple anchor frame detection\n",
      "mechanism based on determining hypothesis divergence be-\n",
      "tween the output and alternative masks.\n",
      "A bounding box is fitted to the output mask and to the\n",
      "union of the output mask and the largest connected compo-\n",
      "nent in the alternative mask. If the ratio between the area of\n",
      "the two bounding boxes drops below θanc = 0.7, the current\n",
      "frame is considered as a potential candidate to update DRM.\n",
      "Note that updating with a grossly incorrectly segmented tar-\n",
      "get would lead to memory corruption and eventual tracking\n",
      "failure. We thus trigger the DRM update only during suf-\n",
      "ficiently stable tracking periods, i.e., if the predicted IoU\n",
      "score from SAM2 exceeds a threshold θIoU = 0.8 and if the\n",
      "3\n",
      "\n",
      "SAM2 memory\n",
      "Distractor-aware memory\n",
      "DRM FIFO\n",
      "RAM FIFO\n",
      "Init\n",
      "Init\n",
      "(not time-stamped)\n",
      "FIFO buﬀer\n",
      "FIXED\n",
      "FIXED\n",
      "t\n",
      "t-1\n",
      "t-2\n",
      "t-3\n",
      "t-4\n",
      "t-5\n",
      "t-6\n",
      "t-1\n",
      "t-Δ\n",
      "t-2Δ\n",
      "t\n",
      "Memory\n",
      "management\n",
      "Always\n",
      "update\n",
      "Figure 2. Overview of the SAM2 memory and the proposed Distractor-Aware Memory (DAM), which splits the model into Recent\n",
      "Appearance Memory (RAM) and Distractor Resolving Memory (DRM) and updates them by a new memory management protocol.\n",
      "mask area is within θarea = 20% of the median area in the\n",
      "last θM = 10 frames. Note that SAM2.1++ is insensitive to\n",
      "the exact value of these parameters.\n",
      "4. A distractor-distilled dataset\n",
      "While benchmarks played a major role in the recent visual\n",
      "object tracking breakthroughs, we note that many of these\n",
      "contain sequences, which are no longer considered chal-\n",
      "lenging by modern standards. In fact, most of the mod-\n",
      "ern trackers obtain high performance on these sequences,\n",
      "which overwhelms the total score and under-represents the\n",
      "improvements on challenging situations. To facilitate the\n",
      "tracking performance analysis of the designs proposed in\n",
      "this paper, we semi-automatically distill several bench-\n",
      "marks into a distractor-distilled tracking dataset (DiDi).\n",
      "We considered validation and test sequences of the ma-\n",
      "jor tracking benchmarks, which are known for high-quality\n",
      "annotation, i.e., GoT-10k [19], LaSOT [15], UTB180 [1],\n",
      "VOT-ST2020 and VOT-LT2020 [22], and VOT-ST2022 and\n",
      "VOT-LT2022 [23]. This gave us a pool of 808 sequences.\n",
      "A sequence was selected for the DiDi dataset if at least one-\n",
      "third of the frames passed the distractor presence criterion\n",
      "described next.\n",
      "A frame was classified as containing non-negligible dis-\n",
      "tractors if it contained a large enough region visually simi-\n",
      "lar to the target. This criterion should be independent from\n",
      "tracker localization method, yet should reflect the power\n",
      "of the modern backbones. We thus encoded the image by\n",
      "DINO2 [34] and for each pixel in the feature space com-\n",
      "puted the distractor score as the average cosine distance to\n",
      "the features within the ground truth target region. We then\n",
      "computed the ratio between the number of pixels outside\n",
      "and inside the target region that exceeded the average of\n",
      "the scores computed within the target region. If the ratio\n",
      "exceeded 0.5, we considered the frame as containing non-\n",
      "negligible distractors.\n",
      "Using the aforementioned protocol, we finally obtained\n",
      "180 sequences with an average sequence length of 1.5k\n",
      "frames (274,882 frames in total). Each sequence contains a\n",
      "single target annotated by an axis-aligned bounding box. In\n",
      "addition, we manually segmented the initial frames to allow\n",
      "the initialization of segmentation-based trackers. Figure 3\n",
      "shows frames from the proposed DiDi dataset. Please see\n",
      "the supplementary material for additional information.\n",
      "5. Experiments\n",
      "The proposed DAM for SAM2 memory model is rigor-\n",
      "ously analyzed.\n",
      "Section 5.1 reports a battery of experi-\n",
      "ments to justify the design choices. Section 5.2 compares\n",
      "the SAM2.1 extension with DAM memory with the state-\n",
      "of-the-art on the DiDi dataset.\n",
      "Detailed analysis on the\n",
      "challenging VOT tracking-by-segmentation benchmarks is\n",
      "reported in Section 5.3, while a comparison on the stan-\n",
      "dard bounding-box tracking benchmarks is reported in Sec-\n",
      "tion 5.4.\n",
      "5.1. Architecture justification\n",
      "The design choices of the proposed distractor-aware mem-\n",
      "ory and the management protocol from Section 3.2 are val-\n",
      "idated on the DiDi dataset from Section 4. We compute\n",
      "the VOTS [24] measures since they simultaneously account\n",
      "for short-term as well as long-term tracking performance.\n",
      "Performance is summarized by the tracking quality Q score\n",
      "and two auxiliary measures: robustness (i.e., portion of suc-\n",
      "cessfully tracked frames) and accuracy (i.e., the average IoU\n",
      "between the prediction and ground truth during successful\n",
      "tracking). Results are shown in Table 1 and in AR plot in\n",
      "Figure 4.\n",
      "We first verify the argument from Section 3.2.1, that up-\n",
      "dating with frames without a target present causes mem-\n",
      "ory degradation. We thus extend SAM2.1, with updating\n",
      "only when the predicted mask is not empty (denoted as\n",
      "SAM2.1PRES). SAM2.1PRES increases the tracking qual-\n",
      "ity Q by 2.5%, primarily by improved robustness, which\n",
      "justifies our claim.\n",
      "We next verify the assumption (also claimed in [51]) that\n",
      "frequent updates reduce tracking robustness due to highly\n",
      "correlated information stored in memory. We reduce the up-\n",
      "date rate in SAM2.1PRES to every 5th frame (SAM2.1∆=5).\n",
      "This negligibly improves Q, but does increase the robust-\n",
      "ness by 1.2%, which supports the claim. We did not observe\n",
      "further performance improvement with increasing ∆.\n",
      "4\n",
      "\n",
      "Figure 3. Example frames from the DiDi dataset showing challenging distractors. Targets are denoted by green bounding boxes.\n",
      "Table 1. SAM2.1++ architecture justification on DiDi dataset.\n",
      "Quality\n",
      "Accuracy\n",
      "Robustness\n",
      "SAM2.1\n",
      "0.649\n",
      "0.720\n",
      "0.887\n",
      "SAM2.1PRES\n",
      "0.665\n",
      "0.723\n",
      "0.903\n",
      "SAM2.1∆=5\n",
      "0.667\n",
      "0.718\n",
      "0.914\n",
      "SAM2.1DRM1\n",
      "0.672\n",
      "0.710\n",
      "0.932\n",
      "SAM2.1DRM2\n",
      "0.644\n",
      "0.691\n",
      "0.913\n",
      "SAM2.1++\n",
      "0.694\n",
      "0.727\n",
      "0.944\n",
      "DRMtenc\n",
      "0.669\n",
      "0.711\n",
      "0.925\n",
      "RAMlast\n",
      "0.685\n",
      "0.724\n",
      "0.932\n",
      "Finally, we focus on the distractor-resolving memory\n",
      "(DRM) part of our proposed memory in Section 3.2.1, that\n",
      "we claim is responsible for the tracking robustness in the\n",
      "presence of distractors. Recall that DRM is updated when a\n",
      "distractor is detected and under the condition that tracking\n",
      "is reliable – we first test the influence of these two condi-\n",
      "tions independently. We thus extend SAM2.1∆=5 with the\n",
      "new DAM memory and update the DRM part only during\n",
      "reliable tracking periods (SAM2.1DRM1). The tracking ac-\n",
      "curacy improves a little, with robustness increase of 2%.\n",
      "Alternatively, we change the rule to update only when the\n",
      "distractor is detected (SAM2.1DRM2). While robustness re-\n",
      "mains unchanged w.r.t. SAM2.1∆=5, the accuracy in fact\n",
      "drops. This is expected since distractor detection may be\n",
      "triggered also by the error in the target segmentation, which\n",
      "gets amplified by the update. To verify this, we next ap-\n",
      "ply both our proposed update DRM rules, arriving to the\n",
      "proposed DAM with SAM2.1 (SAM2.1++ for short). Com-\n",
      "pared to SAM2.1∆=5, we observe substantial improvement\n",
      "in tracking quality Q (4%), primarily due to 3.3% boost in\n",
      "robustness as well as 1.3% boost in accuracy, taking the top-\n",
      "right position in the AR plot (Figure 4) among all variants.\n",
      "This conclusively verifies that DRM should be updated with\n",
      "distractor detected only if tracking is sufficiently reliable.\n",
      "In Section 3.2.1 we claim that DRM part of the mem-\n",
      "ory should not be time-stamped, since the frame influence\n",
      "on the distractor resolving in the current frame should not\n",
      "be biased by the temporal proximity and should serve as a\n",
      "time-less prior. To test this claim, we modify SAM2.1++ by\n",
      "using temporal encodings in DRM (except for the initializa-\n",
      "tion frame) – we denote it as DRMtenc. The tracking quality\n",
      "drops by 3.6%, which confirms the claim. We further in-\n",
      "spect the updating regime in RAM, which always includes\n",
      "the most recent frame, but updates the memory slots every\n",
      "5th frame. SAM2.1++ is modified to update all RAM slots\n",
      "at every 5th frame (RAMlast). This results in a slight track-\n",
      "ing quality decrease (1.3%), which indicates that including\n",
      "the most recent frame in RAM is indeed beneficial, but not\n",
      "critical.\n",
      "0.88\n",
      "0.89\n",
      "0.90\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.68\n",
      "0.69\n",
      "0.70\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "SAM2.1++ [0.694]\n",
      "last [0.685]\n",
      "DRM1 [0.672]\n",
      "tenc [0.669]\n",
      "Δ=5 [0.667]\n",
      "PRES [0.665]\n",
      "SAM2.1 [0.649]\n",
      "DRM2 [0.644]\n",
      "Robustness\n",
      "Accuracy\n",
      "SAM2.1++\n",
      "SAM2.1\n",
      "DRM1\n",
      "DRM2\n",
      "last\n",
      "tenc\n",
      "Δ=5\n",
      "PRES\n",
      "Figure 4. Accuracy-robustness plot on DiDi for the ablated ver-\n",
      "sions of SAM2.1++. The tracking quality is given at each label.\n",
      "5.2. SoTa comparison on DiDi\n",
      "SAM2.1++ is compared on the DiDi dataset with the recent\n",
      "state-of-the-art trackers TransT [5], SeqTrack [6], AQA-\n",
      "Track [40] and AOT [47] as well as trackers with explicit\n",
      "distractor handling mechanisms: KeepTrack [32], Cutie [9]\n",
      "and ODTrack [50]. For completion, we include the most\n",
      "recent, yet unpublished, tracker with improved long-term\n",
      "memory update for SAM2, named SAM2.1Long [14].\n",
      "Results in Table 2 reveal the advantages of the track-\n",
      "5\n",
      "\n",
      "ers with an explicit distractor-handling mechanism over\n",
      "the other trackers. Consider two similarly complex recent\n",
      "trackers SeqTrack and ODTrack, which are both based on\n",
      "the ViT-L backbone. On classical benchmarks like LaSoT,\n",
      "LaSoText and GoT10k, ODTrack outperforms SeqTrack by\n",
      "2%, 6% and 4%, respectively (see Table 6). However, the\n",
      "performance gap increases to 15% on DiDi (Table 2), which\n",
      "confirms that distractors are indeed a major challenge for\n",
      "modern trackers and that DiDi has a unique ability to em-\n",
      "phasize the tracking capability under these conditions and\n",
      "to expose tracking design weaknesses.\n",
      "Focusing on the evaluation of the proposed tracker,\n",
      "SAM2.1++ outperforms all trackers – the standard state-\n",
      "of-the-art trackers as well as trackers with explicit distrac-\n",
      "tor handling mechanism.\n",
      "In particular, SAM2.1++ out-\n",
      "performs state-of-the-art distractor-aware trackers ODTrack\n",
      "and Cutie, by 14% and 21%, respectively.\n",
      "We compare the proposed SAM2.1++ with the concur-\n",
      "rent, unpublished work SAMURAI [45]. SAMURAI is also\n",
      "built on top of SAM2.1 [36], focuses on handling distractors\n",
      "and improves memory management by incorporating mo-\n",
      "tion cues into memory selection and mask refinement pro-\n",
      "cess. In this respect, the work is closely related to ours.\n",
      "Results show that SAM2.1++ outperforms SAMURAI in\n",
      "tracking quality by 2% on DiDi, mostly due to the higher\n",
      "robustness (i.e., SAM2.1++ tracks longer than SAMURAI).\n",
      "This result demonstrates the superiority of our new DAM\n",
      "memory and the management protocol for distractor han-\n",
      "dling, which is also less complex than the concurent coun-\n",
      "terpart proposed in SAMURAI.\n",
      "Compared to another unpublished tracker with the alter-\n",
      "native memory design SAM2.1Long [14], SAM2.1++ out-\n",
      "performs it by a healthy 7% tracking quality boost, indi-\n",
      "cating superiority of our proposed memory. The results re-\n",
      "veal that the major source of the performance boost is the\n",
      "SAM2.1++ tracking robustness, which means it fails less\n",
      "often and thus much better handles distractors. In fact, a\n",
      "close inspection of the results shows that SAM2.1Long per-\n",
      "forms on par with the baseline SAM2.1, which indicates\n",
      "that the long-term memory update mechanism presented\n",
      "in [14] does not improve performance in the presence of\n",
      "distractors. Finally, comparing SAM2.1++ to the baseline\n",
      "SAM2.1 reveals 7% boost in tracking quality, again, at-\n",
      "tributed mostly due to the improved robustness (6%).\n",
      "These results validate the benefits of the proposed DAM\n",
      "memory and its management protocol in handling challeng-\n",
      "ing distractors. Qualitative results of tracking and segmen-\n",
      "tation with SAM2.1++ on DiDi (Figure 5) further demon-\n",
      "strate the remarkable tracking capability in the presence of\n",
      "challenging distractors.\n",
      "Table 2. State-of-the-art comparison on DiDi dataset.\n",
      "Quality\n",
      "Accuracy\n",
      "Robustness\n",
      "SAMURAI [45]\n",
      "0.680 2\n",
      "0.722 3\n",
      "0.930 2\n",
      "SAM2.1Long [14]\n",
      "0.646\n",
      "0.719\n",
      "0.883\n",
      "ODTrack [50]\n",
      "0.608\n",
      "0.740 1\n",
      "0.809\n",
      "Cutie [9]\n",
      "0.575\n",
      "0.704\n",
      "0.776\n",
      "AOT [47]\n",
      "0.541\n",
      "0.622\n",
      "0.852\n",
      "AQATrack [40]\n",
      "0.535\n",
      "0.693\n",
      "0.753\n",
      "SeqTrack [6]\n",
      "0.529\n",
      "0.714\n",
      "0.718\n",
      "KeepTrack [32]\n",
      "0.502\n",
      "0.646\n",
      "0.748\n",
      "TransT [5]\n",
      "0.465\n",
      "0.669\n",
      "0.678\n",
      "SAM2.1 [36]\n",
      "0.649 3\n",
      "0.720\n",
      "0.887 3\n",
      "SAM2.1++\n",
      "0.694 1\n",
      "0.727 2\n",
      "0.944 1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "Figure 5. SAM2.1++ qualitative results on the DiDi dataset with\n",
      "predicted masks shown in green, and tracked objects denoted by\n",
      "arrows. Per-frame overlaps are shown above the figures to indicate\n",
      "failure-free tracking over the entire sequence.\n",
      "5.3. SoTa comparison on VOT benchmarks\n",
      "The VOT initiative [21] is the major tracking initiative, pro-\n",
      "viding challenging datasets for their yearly challenges. In\n",
      "contrast to most of the tracking benchmarks, the targets are\n",
      "annotated by segmentation masks, which allows more ac-\n",
      "curate evaluation of segmentation trackers, compared to the\n",
      "classic bounding-box benchmarks.\n",
      "In this paper, we in-\n",
      "clude two recent single-target challenges: VOT2020 [22]\n",
      "and VOT2022 [23], as well as the most recent multi-target\n",
      "challenge VOTS2024 [25].\n",
      "6\n",
      "\n",
      "Table 3. State-of-the-art comparison on the VOT2020 benchmark.\n",
      "The challenge winner is marked by\n",
      ".\n",
      "EAO\n",
      "Accuracy\n",
      "Robustness\n",
      "ODTrack [50]\n",
      "0.605 3\n",
      "0.761\n",
      "0.902 3\n",
      "MixViT-L+AR [11]\n",
      "0.584\n",
      "0.755\n",
      "0.890\n",
      "SeqTrack-L [6]\n",
      "0.561\n",
      "-\n",
      "-\n",
      "MixFormer-L [10]\n",
      "0.555\n",
      "0.762 3\n",
      "0.855\n",
      "RPT\n",
      "[31]\n",
      "0.530\n",
      "0.700\n",
      "0.869\n",
      "OceanPlus [49]\n",
      "0.491\n",
      "0.685\n",
      "0.842\n",
      "AlphaRef [44]\n",
      "0.482\n",
      "0.754\n",
      "0.777\n",
      "AFOD [7]\n",
      "0.472\n",
      "0.713\n",
      "0.795\n",
      "SAM2.1 [36]\n",
      "0.681 2\n",
      "0.778 2\n",
      "0.941 2\n",
      "SAM2.1++\n",
      "0.729 1\n",
      "0.799 1\n",
      "0.961 1\n",
      "The VOT2020 benchmark [22] consists of 60 challeng-\n",
      "ing sequences, while trackers are run using anchor-based\n",
      "protocol [22] to maximally utilize each sequence. Tracking\n",
      "performance is measured by the accuracy and robustness,\n",
      "summarized by the primary measure called the expected av-\n",
      "erage overlap (EAO).\n",
      "Results on VOT2020 are shown in Table 3. The proposed\n",
      "SAM2.1++ outperforms all compared trackers. In partic-\n",
      "ular, it outperforms the recently published MixViT [11]\n",
      "by 25% EAO, improving both, accuracy and robustness.\n",
      "SAM2.1++ outperforms also the VOT2020 challenge win-\n",
      "ner RPT [31] by a large margin (37.5% in EAO). Comparing\n",
      "SAM2.1++ to the original SAM2.1, the EAO is boosted by\n",
      "7%, while accuracy and robustness are improved by 2.7%\n",
      "and 2.1%, respectively.\n",
      "The VOT2022 benchmark [23] uses a refreshed dataset\n",
      "with 62 sequences (simplest sequences removed, and more\n",
      "challenging added).\n",
      "Table 4 includes the challenge top-\n",
      "performers, including the winner MS AOT [47], as well\n",
      "as the recent published state-of-the-art trackers: Diffusion-\n",
      "Track [30], MixFormer [10], OSTrack [48] and D3Sv2 [28].\n",
      "The proposed SAM2.1++ outperforms the VOT2022 win-\n",
      "ner MS AOT by a significant margin of 12% in EAO.\n",
      "Note that the performance improvement is a consequence\n",
      "of both improved accuracy (2%) and robustness (3%) com-\n",
      "pared to the MS AOT. In addition to achieving state-of-the-\n",
      "art performance, SAM2.1++ outperforms also the baseline\n",
      "SAM2.1 by a healthy margin of 9% EAO.\n",
      "The results on both, VOT2020 and VOT2022 clearly\n",
      "show that SAM2.1++ outperforms all trackers, including\n",
      "the challenges top-performers and the recently published\n",
      "trackers, setting new state-of-the-art on these benchmarks.\n",
      "Despite its simplicity, the proposed training-free memory\n",
      "management is the key element for achieving excellent\n",
      "tracking performance.\n",
      "VOTS2024 benchmark.\n",
      "For a complete evaluation\n",
      "Table 4. State-of-the-art comparison on the VOT2022 benchmark.\n",
      "The challenge winner is marked by\n",
      ".\n",
      "EAO\n",
      "Accuracy\n",
      "Robustness\n",
      "MS AOT\n",
      "[47]\n",
      "0.673 3\n",
      "0.781\n",
      "0.944 3\n",
      "DiffusionTrack [30]\n",
      "0.634\n",
      "-\n",
      "-\n",
      "DAMTMask [23]\n",
      "0.624\n",
      "0.796 3\n",
      "0.891\n",
      "MixFormerM [10]\n",
      "0.589\n",
      "0.799 2\n",
      "0.878\n",
      "OSTrackSTS [48]\n",
      "0.581\n",
      "0.775\n",
      "0.867\n",
      "Linker [42]\n",
      "0.559\n",
      "0.772\n",
      "0.861\n",
      "SRATransTS [23]\n",
      "0.547\n",
      "0.743\n",
      "0.866\n",
      "TransT M [5]\n",
      "0.542\n",
      "0.743\n",
      "0.865\n",
      "GDFormer [23]\n",
      "0.538\n",
      "0.744\n",
      "0.861\n",
      "TransLL [23]\n",
      "0.530\n",
      "0.735\n",
      "0.861\n",
      "LWL B2S [3]\n",
      "0.516\n",
      "0.736\n",
      "0.831\n",
      "D3Sv2 [28]\n",
      "0.497\n",
      "0.713\n",
      "0.827\n",
      "SAM2.1 [36]\n",
      "0.692 2\n",
      "0.779\n",
      "0.946 2\n",
      "SAM2.1++\n",
      "0.753 1\n",
      "0.800 1\n",
      "0.969 1\n",
      "on VOT, we report the performance also on the most re-\n",
      "cent VOTS2024 benchmark. In contrast to VOT2020 and\n",
      "VOT2022, the VOTS2024 benchmark introduces a new,\n",
      "larger dataset, tracking multiple objects in the same scene\n",
      "(with ground truth sequestered at an evaluation server) and a\n",
      "new performance measure, designed to address short-term,\n",
      "long-term3, single and multi-target tracking scenarios. The\n",
      "VOTS2024 is currently considered as the most challenging\n",
      "tracking benchmark.\n",
      "Results are reported in Table 5.\n",
      "It is worth pointing\n",
      "out that the top performers are mostly unpublished (not\n",
      "peer-reviewed) trackers, tuned for the competition, and\n",
      "often complex ad-hoc combinations of multiple methods.\n",
      "For example, the challenge winner S3-Track combines vi-\n",
      "sual and (mono)depth features, uses several huge back-\n",
      "bones, and is much more complex than SAM2.1++. De-\n",
      "spite this, using the same parameters as in other experi-\n",
      "ments, SAM2.1++ achieves a solid second place among the\n",
      "challenging VOTS2024 competition. In particular, it out-\n",
      "performs the challenge-tuned versions of the recently pub-\n",
      "lished trackers LORAT [27], Cutie [9], and the VOT2022\n",
      "winner AOT [47]. Furthermore, the proposed memory man-\n",
      "agement mechanism in SAM2.1++ contributes to 8% per-\n",
      "formance boost in tracking quality compared to the baseline\n",
      "SAM2.1, mostly due to 9% higher robustness.\n",
      "5.4. SoTa comparison on bounding box benchmarks\n",
      "For a complete evaluation, we compare SAM2.1++ on the\n",
      "following three standard bounding box tracking datasets:\n",
      "LaSoT [15], LaSoText [16] and GoT10k [19]. Since frames\n",
      "are annotated by bounding boxes and SAM2 requires a seg-\n",
      "3The target may leave the field of view and re-appear later in the video.\n",
      "7\n",
      "\n",
      "Table 5. State-of-the-art comparison on the VOT2024 benchmark.\n",
      "The challenge winner is marked by\n",
      ".\n",
      "Quality\n",
      "Accuracy\n",
      "Robustness\n",
      "S3-Track\n",
      "[26]\n",
      "0.722 1\n",
      "0.784\n",
      "0.889 1\n",
      "DMAOT SAM [46]\n",
      "0.653\n",
      "0.794 1\n",
      "0.780\n",
      "HQ-DMAOT [46]\n",
      "0.639\n",
      "0.754\n",
      "0.790\n",
      "DMAOT [46]\n",
      "0.636\n",
      "0.751\n",
      "0.795 3\n",
      "LY-SAM [25]\n",
      "0.631\n",
      "0.765\n",
      "0.776\n",
      "Cutie-SAM [9]\n",
      "0.607\n",
      "0.756\n",
      "0.730\n",
      "AOT [47]\n",
      "0.550\n",
      "0.698\n",
      "0.767\n",
      "LORAT [27]\n",
      "0.536\n",
      "0.725\n",
      "0.784\n",
      "SAM2.1 [36]\n",
      "0.661 3\n",
      "0.791 3\n",
      "0.790\n",
      "SAM2.1++\n",
      "0.711 2\n",
      "0.793 2\n",
      "0.864 2\n",
      "mentation mask provided in the first frame, we use the same\n",
      "SAM2 model to estimate the initialization mask. The min-\n",
      "max operation is applied on the predicted masks to obtain\n",
      "the axis-aligned bounding boxes required for the evalua-\n",
      "tion.\n",
      "Tracking performance is computed using area un-\n",
      "der the success rate curve [39] (AUC) in LaSoT [15] and\n",
      "LaSoText [16] and the average overlap [19] (AO) in Got10k.\n",
      "LaSoT [15] is a large-scale tracking dataset with 1,400\n",
      "video sequences, with 280 evaluation sequences and the rest\n",
      "are used for training. The sequences are equally split into\n",
      "70 categories, where each category is represented by 20 se-\n",
      "quences (16 for training and 4 for evaluation). The dataset\n",
      "consists of various scenarios covering short-term and long-\n",
      "term tracking. Results are shown in Table 6. The proposed\n",
      "SAM2.1++ outperforms the baseline SAM2.1 [36] by 7.3%,\n",
      "which indicates that the proposed memory management is\n",
      "important also in a bounding box tracking setup. Further-\n",
      "more, SAM2.1++ performs on par with the top-performing\n",
      "tracker LORAT [27], which was tuned on LaSoT train-\n",
      "ing set, i.e., on the categories included in the evaluation\n",
      "set. It is worth noting that LORAT [27] has approximately\n",
      "50% more training parameters than SAM2.1++, making the\n",
      "model significantly more complex.\n",
      "LaSoText [16] is an extension of the LaSoT [15] dataset,\n",
      "by 150 test sequences, divided into 15 new categories,\n",
      "which are not present in the training dataset. The results in\n",
      "Table 6 show that SAM2.1++ outperforms the baseline ver-\n",
      "sion by a comfortable margin of 7% in AUC. In addition, it\n",
      "outperforms the second-best tracker LORAT [27] by 7.6%.\n",
      "This indicates that SAM2.1++ generalizes well across vari-\n",
      "ous object categories while existing trackers suffer a much\n",
      "larger performance drop.\n",
      "GoT10k [19] is another widely used large-scale track-\n",
      "ing dataset, composed of approximately 10k video se-\n",
      "quences, from which 180 sequences are used for tracking\n",
      "evaluation. We observed that top-performing trackers on\n",
      "GoT10k test set achieve excellent tracking performance,\n",
      "e.g., more than 78% of average overlap, which leaves only\n",
      "small room for potential improvements. However, a solid\n",
      "3.7% boost in tracking performance is observed when com-\n",
      "paring SAM2.1++ to the top-performers LORAT [27] and\n",
      "ODTrack [50]. A close inspection of the SAM2.1++ re-\n",
      "sults reveals that more than 99% of frames are successfully\n",
      "tracked (i.e., with a non-zero overlap), which indicates that\n",
      "GoT10k [19] difficulty level is indeed diminishing for mod-\n",
      "ern trackers.\n",
      "Table 6. State-of-the-art comparison on three standard bounding-\n",
      "box benchmarks.\n",
      "LaSoT\n",
      "LaSoText\n",
      "GoT10k\n",
      "(AUC)\n",
      "(AUC)\n",
      "(AO)\n",
      "MixViT [11]\n",
      "72.4\n",
      "-\n",
      "75.7\n",
      "LORAT [27]\n",
      "75.1 1\n",
      "56.6 3\n",
      "78.2 3\n",
      "ODTrack [50]\n",
      "74.0 2\n",
      "53.9\n",
      "78.2 3\n",
      "DiffusionTrack [30]\n",
      "72.3\n",
      "-\n",
      "74.7\n",
      "DropTrack [38]\n",
      "71.8\n",
      "52.7\n",
      "75.9\n",
      "SeqTrack [6]\n",
      "72.5 3\n",
      "50.7\n",
      "74.8\n",
      "MixFormer [10]\n",
      "70.1\n",
      "-\n",
      "71.2\n",
      "GRM-256 [17]\n",
      "69.9\n",
      "-\n",
      "73.4\n",
      "ROMTrack [4]\n",
      "71.4\n",
      "51.3\n",
      "74.2\n",
      "OSTrack [48]\n",
      "71.1\n",
      "50.5\n",
      "73.7\n",
      "KeepTrack [32]\n",
      "67.1\n",
      "48.2\n",
      "-\n",
      "TOMP [33]\n",
      "68.5\n",
      "-\n",
      "-\n",
      "SAM2.1 [36]\n",
      "70.0\n",
      "56.9 2\n",
      "80.7 2\n",
      "SAM2.1++\n",
      "75.1 1\n",
      "60.9 1\n",
      "81.1 1\n",
      "6. Conclusion\n",
      "We proposed a new distractor-aware memory model DAM\n",
      "and a management regime for memory-based trackers. The\n",
      "new model divides the memory by its tracking function-\n",
      "ality into the recent appearances memory (RAM) and a\n",
      "distractor-resolving memory (DRM), responsible for the\n",
      "tracking accuracy and robustness, respectively. Efficient up-\n",
      "date rules are proposed, which also utilize the tracker out-\n",
      "put to detect frames with critical distractors useful for up-\n",
      "dating DRM. In addition, a distractor-distilled dataset DiDi\n",
      "is proposed to facilitate studying tracking in challenging\n",
      "scenarios.\n",
      "The proposed DAM memory is implemented with\n",
      "SAM2.1 [36], forming SAM2.1++. Extensive analysis con-\n",
      "firms the design decisions. Without any retraining and us-\n",
      "ing fixed parameters, SAM2.1++ sets a solid state-of-the art\n",
      "on six benchmarks with a moderate (20%) speed reduction\n",
      "compared to SAM2.1 (i.e., 11 fps vs 13.3 fps). This makes a\n",
      "compelling case for arguably simpler localization architec-\n",
      "tures of memory-based frameworks compared to the current\n",
      "8\n",
      "\n",
      "tracking state-of-the-art. Furthermore, the results suggest\n",
      "more research should focus on efficient memory designs,\n",
      "with possibly learnable management policies. We believe\n",
      "these directions hold a strong potential for further perfor-\n",
      "mance boosts in future work.\n",
      "Acknowledgements.\n",
      "This work was supported by\n",
      "Slovenian research agency program P2-0214 and projects\n",
      "J2-2506, L2-3169, Z2-4459 and COMET, and by super-\n",
      "computing network SLING (ARNES, EuroHPC Vega -\n",
      "IZUM).\n",
      "References\n",
      "[1] Basit Alawode, Yuhang Guo, Mehnaz Ummar, Naoufel\n",
      "Werghi, Jorge Dias, Ajmal Mian, and Sajid Javed. Utb180:\n",
      "A high-quality benchmark for underwater tracking. In Pro-\n",
      "ceedings of the Asian Conference on Computer Vision, pages\n",
      "3326–3342, 2022. 4, 13\n",
      "[2] Luca Bertinetto, Jack Valmadre, Jo˜ao F Henriques, Andrea\n",
      "Vedaldi, and Philip H S Torr. Fully-convolutional siamese\n",
      "networks for object tracking. In European Conference on\n",
      "Computer Vision Workshops, pages 850–865, 2016. 1\n",
      "[3] Goutam Bhat, Felix J¨aremo Lawin, Martin Danelljan, An-\n",
      "dreas Robinson, Michael Felsberg, Luc Van Gool, and Radu\n",
      "Timofte. Learning what to learn for video object segmenta-\n",
      "tion. In Computer Vision–ECCV 2020: 16th European Con-\n",
      "ference, Glasgow, UK, August 23–28, 2020, Proceedings,\n",
      "Part II 16, pages 777–794. Springer, 2020. 7\n",
      "[4] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu.\n",
      "Ro-\n",
      "bust object modeling for visual tracking. In Proceedings of\n",
      "the IEEE/CVF International Conference on Computer Vision\n",
      "(ICCV), pages 9589–9600, 2023. 8\n",
      "[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,\n",
      "and Huchuan Lu. Transformer tracking. In Comp. Vis. Patt.\n",
      "Recognition, pages 8126–8135, 2021. 1, 2, 5, 6, 7\n",
      "[6] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han\n",
      "Hu. Seqtrack: Sequence to sequence learning for visual ob-\n",
      "ject tracking. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pages 14572–\n",
      "14581, 2023. 1, 2, 5, 6, 7, 8\n",
      "[7] Yiwei Chen, Jingtao Xu, Jiaqian Yu, Qiang Wang, ByungIn\n",
      "Yoo, and Jae-Joon Han. Afod: Adaptive focused discrimina-\n",
      "tive segmentation tracker. In European Conference on Com-\n",
      "puter Vision, pages 666–682. Springer, 2020. 7\n",
      "[8] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\n",
      "term video object segmentation with an atkinson-shiffrin\n",
      "memory model. In European Conference on Computer Vi-\n",
      "sion, pages 640–658. Springer, 2022. 1, 2, 3\n",
      "[9] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young\n",
      "Lee, and Alexander Schwing. Putting the object back into\n",
      "video object segmentation. In Comp. Vis. Patt. Recognition,\n",
      "pages 3151–3161, 2024. 1, 2, 5, 6, 7, 8\n",
      "[10] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu.\n",
      "Mixformer: End-to-end tracking with iterative mixed atten-\n",
      "tion. In Comp. Vis. Patt. Recognition, pages 13608–13618,\n",
      "2022. 1, 7, 8\n",
      "[11] Yutao Cui, Cheng Jiang, Gangshan Wu, and Limin Wang.\n",
      "Mixformer: End-to-end tracking with iterative mixed at-\n",
      "tention.\n",
      "IEEE Trans. Pattern Anal. Mach. Intell., 46(6):\n",
      "4129–4146, 2024. 7, 8\n",
      "[12] Martin Danelljan, Goutam Bhat, Luc Van Gool, and Radu\n",
      "Timofte. Learning discriminative model prediction for track-\n",
      "ing. In Int. Conf. Computer Vision, pages 6181–6190, 2019.\n",
      "2\n",
      "[13] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,\n",
      "Philip HS Torr, and Song Bai. Mose: A new dataset for video\n",
      "object segmentation in complex scenes. In Proceedings of\n",
      "the IEEE/CVF International Conference on Computer Vi-\n",
      "sion, pages 20224–20234, 2023. 2\n",
      "[14] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang\n",
      "Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi\n",
      "Wang.\n",
      "Sam2long: Enhancing sam 2 for long video seg-\n",
      "mentation with a training-free memory tree. arXiv preprint\n",
      "arXiv:2410.16268, 2024. 2, 5, 6\n",
      "[15] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\n",
      "Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\n",
      "Lasot: A high-quality benchmark for large-scale single ob-\n",
      "ject tracking. In Comp. Vis. Patt. Recognition, 2019. 4, 7, 8,\n",
      "13\n",
      "[16] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu,\n",
      "Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu,\n",
      "et al. Lasot: A high-quality large-scale single object tracking\n",
      "benchmark. International Journal of Computer Vision, 129:\n",
      "439–461, 2021. 7, 8\n",
      "[17] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized\n",
      "relation modeling for transformer tracking. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 18686–18695, 2023. 8\n",
      "[18] Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan,\n",
      "Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu\n",
      "Chen, Shuyong Gao, et al. Lvos: A benchmark for large-\n",
      "scale long-term video object segmentation. arXiv preprint\n",
      "arXiv:2404.19326, 2024. 2\n",
      "[19] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k: A\n",
      "large high-diversity benchmark for generic object tracking in\n",
      "the wild. IEEE Trans. Pattern Anal. Mach. Intell., 2019. 4,\n",
      "7, 8, 13\n",
      "[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\n",
      "Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\n",
      "head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and\n",
      "Ross Girshick. Segment anything. In Int. Conf. Computer\n",
      "Vision, pages 4015–4026, 2023. 2\n",
      "[21] M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. Pflugfelder,\n",
      "G. Fernandez, G. Nebehay, F. Porikli, and L. Cehovin. A\n",
      "novel performance evaluation methodology for single-target\n",
      "trackers. IEEE Trans. Pattern Anal. Mach. Intell., 2016. 6\n",
      "[22] Matej Kristan, Aleˇs Leonardis, Jiˇr´ı Matas, Michael Fels-\n",
      "berg, Roman Pflugfelder, Joni-Kristian K¨am¨ar¨ainen, Martin\n",
      "Danelljan, Luka ˇCehovin Zajc, Alan Lukeˇziˇc, Ondrej Dr-\n",
      "bohlav, et al. The eighth visual object tracking VOT2020\n",
      "challenge results. In European Conference on Computer Vi-\n",
      "sion Workshops, 2020. 4, 6, 7, 13\n",
      "[23] Matej Kristan, Aleˇs Leonardis, Jiˇr´ı Matas, Michael Felsberg,\n",
      "Roman Pflugfelder, Joni-Kristian K¨am¨ar¨ainen, Hyung Jin\n",
      "9\n",
      "\n",
      "Chang, Martin Danelljan, Luka ˇCehovin Zajc, Alan Lukeˇziˇc,\n",
      "et al. The tenth visual object tracking vot2022 challenge re-\n",
      "sults. In European Conference on Computer Vision, pages\n",
      "431–460. Springer, 2022. 1, 2, 4, 6, 7, 12, 13\n",
      "[24] Matej Kristan, Jiˇr´ı Matas, Martin Danelljan, Michael Fels-\n",
      "berg, Hyung Jin Chang, Luka ˇCehovin Zajc, Alan Lukeˇziˇc,\n",
      "Ondrej Drbohlav, Zhongqun Zhang, Khanh-Tung Tran, et al.\n",
      "The first visual object tracking segmentation vots2023 chal-\n",
      "lenge results. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 1796–1818, 2023. 4\n",
      "[25] Matej Kristan, Jiri Matas, Pavel Tokmakov, Michael Fels-\n",
      "berg, Luka ˇCehovin Zajc, Alan Lukeˇziˇc, Khanh-Tung Tran,\n",
      "Xuan-Son Vu, Johanna Bjorklund, Hyung Jin Chang, and\n",
      "Gustavo Fernandez. The second visual object tracking seg-\n",
      "mentation vots2024 challenge results, 2024. 1, 2, 6, 8\n",
      "[26] Xin Li, Deshui Miao, Zhenyu He, Yaowei Wang, Huchuan\n",
      "Lu, and Ming-Hsuan Yang. Learning spatial-semantic fea-\n",
      "tures for robust video object segmentation. arXiv preprint\n",
      "arXiv:2407.07760, 2024. 8\n",
      "[27] Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong\n",
      "Xu, and Haibin Ling. Tracking meets lora: Faster training,\n",
      "larger model, stronger performance. In ECCV, 2024. 7, 8\n",
      "[28] Alan Lukeˇziˇc, Jiˇr´ı Matas, and Matej Kristan. A discrimina-\n",
      "tive single-shot segmentation network for visual object track-\n",
      "ing. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 44(12):9742–9755, 2021. 1, 7\n",
      "[29] Alan Lukeˇziˇc, ˇZiga Trojer, Jiˇr´ı Matas, and Matej Kristan.\n",
      "A new dataset and a distractor-aware architecture for trans-\n",
      "parent object tracking. International Journal of Computer\n",
      "Vision, pages 1–14, 2024. 1, 2\n",
      "[30] Run Luo, Zikai Song, Lintao Ma, Jinlin Wei, Wei Yang, and\n",
      "Min Yang. Diffusiontrack: Diffusion model for multi-object\n",
      "tracking. In Proceedings of the AAAI Conference on Artifi-\n",
      "cial Intelligence, pages 3991–3999, 2024. 7, 8\n",
      "[31] Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu, and Jun\n",
      "Yin. Rpt: Learning point set representation for siamese vi-\n",
      "sual tracking. In Computer Vision–ECCV 2020 Workshops:\n",
      "Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16,\n",
      "pages 653–665. Springer, 2020. 7\n",
      "[32] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and\n",
      "Luc Van Gool. Learning target candidate association to keep\n",
      "track of what not to track. In Proceedings of the IEEE/CVF\n",
      "international conference on computer vision, pages 13444–\n",
      "13454, 2021. 2, 5, 6, 8\n",
      "[33] Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu\n",
      "Paul, Danda Pani Paudel, Fisher Yu, and Luc Van Gool.\n",
      "Transforming model prediction for tracking. In Comp. Vis.\n",
      "Patt. Recognition, 2022. 2, 8\n",
      "[34] Maxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V.\n",
      "Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\n",
      "Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\n",
      "sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\n",
      "Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\n",
      "las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\n",
      "Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\n",
      "janowski. Dinov2: Learning robust visual features without\n",
      "supervision. arXiv:2304.07193, 2023. 4\n",
      "[35] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\n",
      "bel´aez, Alexander Sorkine-Hornung, and Luc Van Gool.\n",
      "The 2017 davis challenge on video object segmentation.\n",
      "arXiv:1704.00675, 2017. 2\n",
      "[36] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\n",
      "Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\n",
      "R¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt-\n",
      "ing Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\n",
      "Yuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Feicht-\n",
      "enhofer. Sam 2: Segment anything in images and videos.\n",
      "arXiv preprint arXiv:2408.00714, 2024. 2, 3, 6, 7, 8, 12\n",
      "[37] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei,\n",
      "Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu\n",
      "Chowdhury, Omid Poursaeed, Judy Hoffman, et al.\n",
      "Hi-\n",
      "era: A hierarchical vision transformer without the bells-and-\n",
      "whistles. In International Conference on Machine Learning,\n",
      "pages 29441–29454. PMLR, 2023. 2\n",
      "[38] Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Baoyuan Wu,\n",
      "Ying Shan, and Antoni B Chan. Dropmae: Masked autoen-\n",
      "coders with spatial-attention dropout for tracking tasks. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vi-\n",
      "sion and Pattern Recognition, pages 14561–14571, 2023. 8\n",
      "[39] Y. Wu, J. Lim, and M. H. Yang. Object tracking benchmark.\n",
      "IEEE Trans. Pattern Anal. Mach. Intell., 37(9):1834–1848,\n",
      "2015. 8\n",
      "[40] Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang,\n",
      "Liangtao Shi, Shuxiang Song, and Rongrong Ji.\n",
      "Autore-\n",
      "gressive queries for adaptive tracking with spatio-temporal\n",
      "transformers. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pages 19300–\n",
      "19309, 2024. 5, 6\n",
      "[41] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\n",
      "Liang, Jianchao Yang, and Thomas Huang.\n",
      "Youtube-vos:\n",
      "A large-scale video object segmentation benchmark. arXiv\n",
      "preprint arXiv:1809.03327, 2018. 2\n",
      "[42] Zizheng Xun, Shangzhe Di, Yulu Gao, Zongheng Tang,\n",
      "Gang Wang, Si Liu, and Bo Li. Linker: Learning long short-\n",
      "term associations for robust visual tracking. IEEE Transac-\n",
      "tions on Multimedia, 26:6228–6237, 2024. 7\n",
      "[43] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and\n",
      "Huchuan Lu.\n",
      "Learning spatio-temporal transformer for\n",
      "visual tracking.\n",
      "In Proceedings of the IEEE/CVF Inter-\n",
      "national Conference on Computer Vision (ICCV), pages\n",
      "10448–10457, 2021. 1, 2\n",
      "[44] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and Xi-\n",
      "aoyun Yang. Alpha-refine: Boosting tracking performance\n",
      "by precise bounding box estimation.\n",
      "In Comp. Vis. Patt.\n",
      "Recognition, pages 5289–5298, 2021. 7\n",
      "[45] Cheng-Yen Yang,\n",
      "Hsiang-Wei Huang,\n",
      "Wenhao Chai,\n",
      "Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting\n",
      "segment anything model for zero-shot visual tracking with\n",
      "motion-aware memory. arXiv:2411.11922, 2024. 6\n",
      "[46] Zongxin Yang and Yi Yang. Decoupling features in hier-\n",
      "archical propagation for video object segmentation. In Ad-\n",
      "vances in Neural Information Processing Systems (NeurIPS),\n",
      "2022. 8\n",
      "[47] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating ob-\n",
      "jects with transformers for video object segmentation. Ad-\n",
      "10\n",
      "\n",
      "vances in Neural Information Processing Systems, 34:2491–\n",
      "2502, 2021. 1, 2, 3, 5, 6, 7, 8\n",
      "[48] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and\n",
      "Xilin Chen. Joint feature learning and relation modeling for\n",
      "tracking: A one-stream framework. In ECCV, 2022. 7, 8\n",
      "[49] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\n",
      "Weiming Hu. Ocean: Object-aware anchor-free tracking. In\n",
      "Computer Vision–ECCV 2020: 16th European Conference,\n",
      "Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI\n",
      "16, pages 771–787. Springer, 2020. 7\n",
      "[50] Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo,\n",
      "Shengping Zhang, and Xianxian Li. Odtrack: Online dense\n",
      "temporal token learning for visual tracking.\n",
      "In Proceed-\n",
      "ings of the AAAI Conference on Artificial Intelligence, pages\n",
      "7588–7596, 2024. 1, 5, 6, 7, 8\n",
      "[51] Junbao Zhou, Ziqi Pang, and Yu-Xiong Wang. Rmem: Re-\n",
      "stricted memory banks improve video object segmentation.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 18602–18611, 2024.\n",
      "1, 2, 3, 4\n",
      "11\n",
      "\n",
      "7. Appendix\n",
      "7.1. Impact of the model size\n",
      "The segment anything model 2 (SAM2) [36] was originally\n",
      "developed in four model sizes, denoted by tiny (T), small\n",
      "(S), base (B) and large (L). In Table 7 these four model sizes\n",
      "of unchanged SAM2.1 are compared with our SAM2.1++\n",
      "version, presented in the paper on the new DiDi dataset.\n",
      "Results show a clear and consistent performance improve-\n",
      "ment across all four model sizes. In particular, the tracking\n",
      "quality improves by approximately 6% or 7%, depending\n",
      "on the model size, mostly due to the improved robustness.\n",
      "These results show that the proposed distractor-aware mem-\n",
      "ory generalizes well over various model sizes, demonstrat-\n",
      "ing that the model has not been tuned to the exact SAM2\n",
      "model.\n",
      "Table 7. Comparison of different model sizes on DiDi dataset.\n",
      "The ++ denotes the proposed tracker, while the T, S, B, L denote\n",
      "the tiny, small, base and large Hiera backbone sizes, respectively.\n",
      "Params denotes number of parameters, while Acc. and Rob. de-\n",
      "note accuracy and robustness, respectively.\n",
      "Params\n",
      "Quality\n",
      "Acc.\n",
      "Rob.\n",
      "SAM2.1-T\n",
      "39M\n",
      "0.600\n",
      "0.697\n",
      "0.848\n",
      "SAM2.1++-T\n",
      "39M\n",
      "0.642 ↑7%\n",
      "0.695\n",
      "0.907\n",
      "SAM2.1-S\n",
      "46M\n",
      "0.630\n",
      "0.718\n",
      "0.866\n",
      "SAM2.1++-S\n",
      "46M\n",
      "0.668 ↑6%\n",
      "0.709\n",
      "0.930\n",
      "SAM2.1-B\n",
      "81M\n",
      "0.624\n",
      "0.721\n",
      "0.856\n",
      "SAM2.1++-B\n",
      "81M\n",
      "0.664 ↑6%\n",
      "0.709\n",
      "0.930\n",
      "SAM2.1-L\n",
      "224M\n",
      "0.649\n",
      "0.720\n",
      "0.887\n",
      "SAM2.1++-L\n",
      "224M\n",
      "0.694 ↑7%\n",
      "0.727\n",
      "0.944\n",
      "7.2. Impact of the model version\n",
      "This section compares the performance improvements for\n",
      "the two individual SAM versions, i.e., SAM2 and SAM2.1.\n",
      "The SAM2.1 version improves the initial version in han-\n",
      "dling small and visually similar objects by introducing ad-\n",
      "ditional augmentation techniques in training.\n",
      "It also in-\n",
      "cludes improved occlusion handling by training the model\n",
      "on longer frame sequences.\n",
      "Results are shown in Table 8.\n",
      "To demonstrate the\n",
      "improvement of the 2.1 model version over version 2,\n",
      "we compare the SAM2 with the SAM2.1 on the DiDi\n",
      "dataset, which results in approximately 3.5% improve-\n",
      "ment in tracking quality.\n",
      "Next, we compare the origi-\n",
      "nal SAM2 and the version with our new memory model\n",
      "(i.e., SAM2++).\n",
      "The tracking performance improves by\n",
      "7%, which is well beyond the performance improvement\n",
      "from SAM2 to SAM2.1 and supports the importance of a\n",
      "high-quality memory model and the memory management\n",
      "regime.\n",
      "A simimlar performance boost (close to 7%) is\n",
      "observed between SAM2.1 and SAM2.1++, which implies\n",
      "complementarity of the new memory model with the base-\n",
      "line method performance imporvements that come from\n",
      "better training. Similarly as in Section 7.1, we conclude that\n",
      "the proposed distractor-aware memory is robust to different\n",
      "model versions, demonstrating a consistent improvements\n",
      "in tracking performance on two SAM2 versions.\n",
      "Table 8. Comparison of two SAM model versions: SAM2 and\n",
      "SAM2.1 on DiDi dataset.\n",
      "Quality\n",
      "Accuracy\n",
      "Robustness\n",
      "SAM2\n",
      "0.627\n",
      "0.723\n",
      "0.850\n",
      "SAM2++\n",
      "0.668 ↑7%\n",
      "0.710\n",
      "0.929\n",
      "SAM2.1\n",
      "0.649\n",
      "0.720\n",
      "0.887\n",
      "SAM2.1++\n",
      "0.694 ↑7%\n",
      "0.727\n",
      "0.944\n",
      "7.3. Real-time performance\n",
      "We further evaluate the proposed SAM2.1++ under real-\n",
      "time tracking constraints. We thus evaluate the tracker on\n",
      "VOT2022-RT [23] challenge4, which was specifically de-\n",
      "signed for real-time evaluation.\n",
      "Specifically, VOT chal-\n",
      "lenges are run by VOT toolkits, which manage the real-time\n",
      "constraints. A frame is sent to the tracker, which needs to\n",
      "process it and report the target position at 20FPS frame rate.\n",
      "If the tracker is not able to process the frame in time, the\n",
      "prediction from the previous frame is used as the estimate\n",
      "for the current frame and next frame is sent to the tracker.\n",
      "Such a setup simulates actual real-time scenario, which is\n",
      "much more realistic than reporting just the average track-\n",
      "ing speed.\n",
      "The tracking performance is measured using\n",
      "standard VOT2022 measures [23]: the primary measure ex-\n",
      "pected average overlap (EAO), and two auxiliary measures,\n",
      "i.e., accuracy and robustness.\n",
      "The results in Table 9 show that the proposed SAM2.1++\n",
      "(L model size) outperforms all trackers that participated in\n",
      "VOT2022-RT challenge. In particular, it outperforms the\n",
      "challenge winner by 4% in EAO demonstrating excellent\n",
      "real-time performance.\n",
      "These results show that the pro-\n",
      "posed distractor-aware memory adds only a small compu-\n",
      "tational overhead, yet bringing remarkable robustness capa-\n",
      "bilities and making it useful for real applications.\n",
      "7.4. Sensitivity to threshold values\n",
      "We analyze the sensitivity of the proposed SAM2.1++ to the\n",
      "exact value of the manually determined parameters. In par-\n",
      "ticular, we focus on three thresholds defined in Distractor-\n",
      "resolving memory (Section 3.2.2). Experiments were con-\n",
      "ducted on the VOT2022 [23] dataset using the unsupervised\n",
      "4SAM2.1 and SAM2.1++ were evaluated on the machine with the\n",
      "AMD EPYC 7763 64-Core 2.45 GHz CPU and Nvidia A100 40GB GPU.\n",
      "12\n",
      "\n",
      "Table 9. Real-time performance on VOT2022-RT challenge. The\n",
      "challenge winner is marked by\n",
      ".\n",
      "EAO\n",
      "Accuracy\n",
      "Robustness\n",
      "MS AOT\n",
      "0.610 3\n",
      "0.751 2\n",
      "0.921 3\n",
      "OSTrackSTS\n",
      "0.569\n",
      "0.766 1\n",
      "0.860\n",
      "SRATransTS\n",
      "0.547\n",
      "0.743 3\n",
      "0.866\n",
      "SAM2.1\n",
      "0.614 2\n",
      "0.722\n",
      "0.922 2\n",
      "SAM2.1++\n",
      "0.635 1\n",
      "0.717\n",
      "0.942 1\n",
      "experiment to ensure fast execution and compute the aver-\n",
      "age overlap (AO) as the performance measure.\n",
      "We analyzed the influence of the threshold for the ra-\n",
      "tio between the target and alternative predicted masks, i.e.\n",
      "Θanc = 0.7. This ratio is used for preemptive update upon\n",
      "distractor detection and is thus crucial for our distractor-\n",
      "aware memory. The results in Figure 6 show that track-\n",
      "ing results are extremely stable for a wide range of Θanc ∈\n",
      "[0.6, 0.9] demonstrating the robust design of the tracker.\n",
      "Next, we tested the IoU score threshold (ΘIou = 0.8),\n",
      "used to determine if the predicted mask is reliable for dis-\n",
      "tractor testing. Results in Figure 6 show that tracking per-\n",
      "formance is extremely stable for a wide range of parameters\n",
      "(ΘIoU ∈[0.5, 0.8]) scoring almost identical AO.\n",
      "Finally, the mask area threshold was tested, i.e. Θarea\n",
      "= 0.2. This threshold is used to determine the tracking sta-\n",
      "bility and is together with the ΘIoU a necessary condition\n",
      "to trigger the DRM update. Results in Figure 6 show stable\n",
      "tracking performance across the wide range of thresholds\n",
      "and confirms that SAM2.1++ is not sensitive to the exact\n",
      "value of this parameter.\n",
      "7.5. DiDi dataset statistics\n",
      "In this section we provide additional information about the\n",
      "distractor-distilled dataset DiDi construction. In particular,\n",
      "number of sequences from each source dataset is given in\n",
      "the following:\n",
      "• LaSoT [15]: 86 sequences\n",
      "• UTB-180 [1]: 56 sequences\n",
      "• VOT2022-ST [23]: 20 sequences\n",
      "• VOT2022-LT [23]: 7 sequences\n",
      "• VOT2020-LT [22]: 6 sequences\n",
      "• GoT10k [19]: 4 sequences\n",
      "• VOT2020-ST [22]: 1 sequences\n",
      "• Total: 180 sequences (274,882 frames)\n",
      "7.6. Qualitative analysis\n",
      "Figure 7 presents a qualitative comparison between the\n",
      "baseline SAM2.1 and the proposed SAM2.1++ on four\n",
      "video sequences. In the first row a zebra is tracked with\n",
      "other zebras in its vicinity. When the zebra is partially oc-\n",
      "0.50\n",
      "0.60\n",
      "0.70\n",
      "0.80\n",
      "0.90\n",
      "0\n",
      "1\n",
      "IoU score threshold\n",
      "0.50\n",
      "0.60\n",
      "0.70\n",
      "0.80\n",
      "0.90\n",
      "0\n",
      "1\n",
      "0.10\n",
      "0.20\n",
      "0.30\n",
      "0.40\n",
      "0.50\n",
      "0\n",
      "1\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "ΘIoU\n",
      "Θanc\n",
      "Area ratio threshold\n",
      "Θarea\n",
      "Mask area threshold\n",
      "Figure 6. Sensitivity of the SAM2.1++ to different values of three\n",
      "thresholds: Θanc, ΘIoU and Θarea. Experiments were done on\n",
      "VOT2022 using average overlap as the performance measure. The\n",
      "selected threshold value is marked by a green circle.\n",
      "cluded, SAM2.1 drifts to the wrong zebra and starts to track\n",
      "it, while SAM2.1++ tracks only the visible part of the target\n",
      "during occlusion and stays on the selected zebra until the\n",
      "end of the sequence.\n",
      "In the second row of Figure 7, the baseline SAM2.1\n",
      "tracker successfully tracks the bus until the full occlusion\n",
      "and fails to re-detect it after the re-appearance. This failure\n",
      "occurs due to the too frequent memory updates when target\n",
      "is occluded and is successfully addressed with the proposed\n",
      "memory update in SAM2.1++.\n",
      "The third row in Figure 7 shows tracking of a flamingo’s\n",
      "head. The baseline SAM2.1 tends to jump on the bird’s\n",
      "beak or extend to the whole body, since it prefers to segment\n",
      "the regions with so-called high objectness (i.e., regions with\n",
      "well-defined edges). The proposed SAM2.1++ successfully\n",
      "tracks the flamingo’s head even if the edge between the head\n",
      "and the neck is not clearly visible. In this case, part of the\n",
      "neck is segmented by an alternative mask and thus detected\n",
      "as a distractor. Updating the distractor resolving memory\n",
      "(DRM) using such critical frames results in a more stable\n",
      "and accurate tracking.\n",
      "A similar effect is demonstrated in the fourth row of Fig-\n",
      "ure 7, where a fish similar to the tracked fish occludes it and\n",
      "causes SAM2.1 to jump to it. On the other hand, SAM2.1++\n",
      "successfully detects such critical frames, updates the DRM\n",
      "and avoids the tracking failure.\n",
      "13\n",
      "\n",
      "≈\n",
      "≈\n",
      "≈\n",
      "≈\n",
      "≈\n",
      "0\n",
      "0\n",
      "≈\n",
      "SAM2.1++\n",
      "SAM2.1\n",
      "Figure 7. Qualitative comparison of the baseline SAM2.1 (blue) and the proposed SAM2.1++ (yellow). The symbol ≈denotes approxi-\n",
      "mately identical outputs and ∅denotes an empty prediction (i.e, mask with all-zeros). Tracked object is denoted with a green arrow.\n",
      "14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pymupdf.open(os.path.join(data_dir, \"A_Distractor-Aware_Memory_for_Visual_Object_Tracking_with_SAM2.pdf\")) as pdf:\n",
    "    for page in pdf:\n",
    "        print(page.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd83e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
