{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f85c05e65c70bc0",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [boshko.koloski@ijs.si](mailto:boshko.koloski@ijs.si) for any comments.</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771ac5b7cccdd00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The core functionality of this notebook is to create a retrieval-augmented generation (RAG) system that enables discussion about the NLP subject setting using the `Mistral-7b-v0.2` model.\n",
    "\n",
    "General-purpose language models can be fine-tuned for common tasks such as sentiment analysis and named entity recognition, which typically do not require additional background knowledge.\n",
    "\n",
    "For more complex and knowledge-intensive tasks, it is possible to construct a language model-based system that accesses external knowledge sources. This approach enhances factual consistency, improves the reliability of responses, and mitigates the issue of 'hallucination.'\n",
    "\n",
    "Researchers have introduced the Retrieval Augmented Generation (RAG) method to address these knowledge-intensive tasks. RAG integrates an information retrieval component with a text generator model, allowing for efficient updates and modifications to its internal knowledge without retraining the entire model.\n",
    "\n",
    "RAG operates by taking an input, retrieving a set of relevant or supporting documents from a source like Wikipedia, and concatenating these documents with the original input prompt. This concatenated context is then fed to the text generator, which produces the final output. This adaptability is crucial for situations where facts may evolve over time, as the static parametric knowledge of traditional large language models (LLMs) can become outdated. RAG circumvents the need for retraining, providing access to the most current information and enabling reliable outputs through retrieval-based generation.\n",
    "\n",
    "RAG requires additional document embeddings and the storage of documents in a database for retrieval purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023bf09e0d6f6d8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the cells below to setup and install the required libraries. For our experiment we will need `bitsandbytes`, `accelerate`, `transformers`, `datasets`, `sentence-transformers` and  `faiss-gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a96eb267c0a6ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:37:53.031853900Z",
     "start_time": "2025-04-22T16:37:46.643660700Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain bitsandbytes accelerate sentence-transformers faiss-cpu langchain_community unstructured python-docx --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8a8e3-3702-4be1-bd8d-35508fd01722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:29:35.886241900Z",
     "start_time": "2025-04-22T16:29:31.332247400Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e3501f-3a53-4b33-a6ac-862d4a00b718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:38:05.275872900Z",
     "start_time": "2025-04-22T16:38:05.261375400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n",
      "True\n",
      "12.1\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())           # should be True\n",
    "print(torch.version.cuda)                  # should show 12.1\n",
    "print(torch.cuda.get_device_name(0))       # should return your GPU name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ba74db4cddd67",
   "metadata": {},
   "source": [
    "**In simple terms, RAG is to LLMs what an open-book exam is to humans.**\n",
    "\n",
    "The concept of an open-book exam centers around assessing a student's reasoning abilities rather than their capacity to memorize specific details. In a similar vein, RAG separates factual knowledge from the LLM’s reasoning capabilities. This factual information is stored in an external knowledge source, which is both easily accessible and updatable:\n",
    "\n",
    "- **Parametric knowledge:** Knowledge that is learned during training and implicitly stored within the neural network's weights.\n",
    "- **Non-parametric knowledge:** Information that is stored externally, for example, in a vector database.\n",
    "e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700794adcf9e90c",
   "metadata": {},
   "source": [
    "![RAG](RAG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354fcc51dcd38f7",
   "metadata": {},
   "source": [
    "The RAG workflow consists of:\n",
    "\n",
    "1. **The Retrieve**: The user query is used to retrieve relevant context from an external knowledge source. For this, the user query is embedded using an embedding model into the same vector space as the additional context in the vector database. This enables a similarity search, and the top k closest data objects from the vector database are returned.\n",
    "2. **Augment**: The user query and the retrieved additional context are incorporated into a prompt template.\n",
    "3. **Generate**: Finally, the retrieval-augmented prompt is fed to the LLM.\n",
    "\n",
    "We will use the `langchain` framework to efficiently prompt the LLMs and prepare the RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90ca0fc832d952b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:38:10.697878500Z",
     "start_time": "2025-04-22T16:38:10.688851400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import requests\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir \"pydantic>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14db7b4f55406a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:40:57.221461200Z",
     "start_time": "2025-04-22T16:40:51.933459100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.3\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "print(pydantic.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d512e20afb17a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:39:31.981053800Z",
     "start_time": "2025-04-22T16:39:31.787536Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter, TokenTextSplitter\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83fba5f29fed05c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:34:46.036350300Z",
     "start_time": "2025-04-22T16:34:46.029718800Z"
    }
   },
   "outputs": [],
   "source": [
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f\"cuda:{torch.cuda.current_device()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66b1984d75e78",
   "metadata": {},
   "source": [
    "Large Language Models are known for their significant computational demands. Typically, the size of a model is determined by multiplying the number of parameters (size) by the precision of these values (data type). To conserve memory, weights can be stored using lower-precision data types through a process known as quantization.\n",
    "\n",
    "**Post-Training Quantization (PTQ)** is a straightforward technique where the weights of an already trained model are converted to a lower precision without necessitating any retraining. Although easy to implement, PTQ can lead to potential performance degradation.We will employ PTQ using the `bitsandbytes` library and will load the model in 4-bit precision, applying double quantization with the `nf4` data type. For more information about quantization, visit [this guide on quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization) )pe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6330616774c702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:34:55.402329Z",
     "start_time": "2025-04-22T16:34:54.961221900Z"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # loading in 4 bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # quantization type\n",
    "    bnb_4bit_use_double_quant=True, # nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b44c55df481fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:04:39.048894Z",
     "start_time": "2025-04-22T15:04:38.216612Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `jupyter-notebook` has been saved to C:\\Users\\Admin\\.cache\\huggingface\\stored_tokens\n",
      "Your token has been saved to C:\\Users\\Admin\\.cache\\huggingface\\token\n",
      "Login successful.\n",
      "The current active token is: `jupyter-notebook`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <INSERT_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a191ae8ad63576cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:29:23.753480Z",
     "start_time": "2025-04-22T15:29:20.662123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282bf43923534873a05f3ed24ff57462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836dd912d04b43a5b112985adc439d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c49a94d3f2463e9515982496fcb1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc4a52465874b7dbab66cb47885658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37973e7f630a449388a4ae98a8fae4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40f78d502d14854a9f5114a06d8a226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd087fa9f2e458aa054220900776b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7860b829a145ceb2db916b3641ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config, # we introduce the bnb config here.\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370171a4b6a86e0d",
   "metadata": {},
   "source": [
    "We also need to load the tokenizer to transform the text as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d085b8b5e7e48df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e2200c38cd449bb880b99435b1edd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d17c5f56c4827afcba98bee3f50b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5c9ed6027f4b2e92441f5e5227f09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f5a6d07437466788180b680497d08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2711c6629b79e65",
   "metadata": {},
   "source": [
    "We will use pipelines from Hugging Face to perform the prompting and generation with the Mistral model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3ebf83db21f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    #temperature=0.0,\n",
    "    max_new_tokens=8192,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90159704c59c8d37",
   "metadata": {},
   "source": [
    "We will use `langchain` to link the HuggingFace models and the chaining prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7a9e634bead5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11056\\3495189882.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56feed9cfad5b",
   "metadata": {},
   "source": [
    "The core functionality of `langchain` is the creation of templates for prompting via `PromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab547fad62f3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b12aeac6f5a8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Reply your answer in markdown format.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24ff19982849058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=\"\\nYou are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nReply your answer in markdown format.\\nQuestion: {question}\\nAnswer:\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f09ab9f78ce4d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2ed18b29de3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=\"\\nYou are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nReply your answer in markdown format.\\nQuestion: {question}\\nAnswer:\")\n",
       "| HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x00000261D33FD410>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1844107785fc10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the scoring criteria of the NLP course?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a12a13b3d7902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Reply your answer in markdown format.\n",
      "Question: What is the scoring criteria of the NLP course?\n",
      "Answer: I'd be happy to help you with that! However, without specific context regarding which NLP (Natural Language Processing) course you're referring to, I can only provide some general information about common scoring criteria used in NLP courses.\n",
      "Here are some common assessment areas for NLP projects and assignments:\n",
      "- **Accuracy**: This measures how often the model or algorithm produces correct results.\n",
      "- **Precision**: This measures the proportion of true positives among the total number of positive predictions made by the model.\n",
      "- **Recall**: This measures the proportion of true positives among all actual positive instances in the data.\n",
      "- **F1 Score**: This is the harmonic mean of precision and recall, providing a balanced measure of both.\n",
      "- **Perplexity**: This measures how well a language model predicts a given text. Lower perplexity scores indicate better performance.\n",
      "- **BLEU (Bilingual Evaluation Understudy) score**: This is commonly used for evaluating machine translation systems. It compares the generated output to a reference sentence using n-gram precision.\n",
      "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score**: This measures the overlap between the generated summary and the reference summary using n-grams.\n",
      "Please let me know if you have more specific information about the NLP course you're asking about, so I can give you a more accurate answer.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc132b9fa6e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do I have to do for the peer review?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c2e1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: huggingface-hub\n",
      "Version: 0.30.2\n",
      "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
      "Home-page: https://github.com/huggingface/huggingface_hub\n",
      "Author: Hugging Face, Inc.\n",
      "Author-email: julien@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, packaging, pyyaml, requests, tqdm, typing-extensions\n",
      "Required-by: accelerate, datasets, sentence-transformers, tokenizers, transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show huggingface_hub --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab746b668538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2d3ba58d14aaa",
   "metadata": {},
   "source": [
    "**NOTE**: The model provides responses that are fairly general. We plan to enhance this by integrating contextual details from our course materials via **RAG**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9240487369c8a24",
   "metadata": {},
   "source": [
    "### Enter RAG\n",
    "\n",
    "Next, we will define the vector embeddings of our context. We will use the `sentence-transformers/all-mpnet-base-v2` model to embed the documents and a FAISS vector store to store and later retrieve them. LangChain offers the `HuggingFaceEmbeddings` interface to easily load any model from Hugging Face to serve as the document representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f58f29d1a6c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1bd4731f85e8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11056\\2346813208.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9757af6ffb437e871b1100d0aeb149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d3a958332643d3ad0dc6916affa465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e99c0054340ed91593054374bcf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9479d967f9481f9d45310b5ea78653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2ddbbe6b924af3a4030793ecfe786d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87b05301a4148eb83ef8bd6d4c9566e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b6801123e746f5aa48789991fdde08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b2dbcc3c64981b3798eb1662ae3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6cd4ba01bc4f9e8fcd023bc64be498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14976c7da9514e478ae2ac36d6983f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2686bfb0fd44ecbd4d22441f50ae1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85a1d43e13aa57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_websites(sites: list[str]):\n",
    "    docs = []\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        filename = f\"{tmpdir}/site.html\"\n",
    "        for site in sites:\n",
    "            res = requests.get(site)\n",
    "            with open(filename, mode=\"wb\") as fp:\n",
    "                fp.write(res.content)\n",
    "            docs.extend(BSHTMLLoader(filename, open_encoding = 'utf8').load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fcf6c3f1ff8c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_webpage = \"https://docs.google.com/document/d/e/2PACX-1vRwReAm5Vz_aiNyZN33eTz22fqMJhM0H-KtJdXthUw5cean_WYdBkZgJchP_s9th0rtUW0ikZZ_Fh5l/pub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f2abe19884a5f",
   "metadata": {},
   "source": [
    "The `fetch_websites` function will be used to scrape data from our Google Document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a013f997c36d2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = fetch_websites([course_webpage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd304f7d3f7c9a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp\\\\tmpyadichtx/site.html', 'title': 'Natural language processing 2025'}, page_content='Natural language processing 2025Opublikowano za pomocą Dokumentów GoogleZgłoś nadużycieWięcej informacjiNatural language processing 2025Automatyczne aktualizacje co 5\\xa0minLaboratory work - Spring 2025The main goal of laboratory work is to present the most important aspects of data science in practice and to teach you how to use key tools for a NLP engineer. We especially emphasize on self-paced work, raising standards related to development, replicability, reporting, research, visualizing, etc. Our goal is not to provide exact instructions or \"make robots\" out of participants of this course. Participants will need to try to navigate themselves among data, identify promising leads and extract as much information as possible from the data to present to the others (colleagues, instructors, companies or their superiors).Important linksLab sessions course repository\\xa0(continuously updated, use weekly plan links for latest materials)Project report templateBooks and other materials\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Speech and language processing\\xa0(online draft)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Python 3 Text Processing with NLTK 3 CookbookIntroduction to Data Science Handbook\\xa0 Razvoj slovenščine v digitalnem okolju\\xa0(February 2023)Previous years NLP course materials NLP course 2024 project reports\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0MarksPeer review\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0TBA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0* Check Marks spreadsheet (sheet \"Projects to review\") to get list of repositories you need to review.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Weekly planThis plan is regularly updated. \\xa0Lab sessions\\xa0are meant to discuss materials and your project ideas. Those proficient in some of the topics covered during the course are expected to help other students during the lab work or in online discussions. Such contributions will also be taken into account. During the lab sessions we will show some DEMOs based on which you will work on your projects. Based on your proposals / new ideas we can adapt the weekly plan and prepare additional materials. All the lab session tutorials will be regularly updated in the Github repository. During the lab sessions we will briefly present each week\\'s topic and then mostly discuss your project ideas and work. You are expected to check/run notebooks before the lab sessions and then ask questions/discuss during the lab sessions. In the repository\\'s README you can also find the recordings of each topic.WeekDescriptionMaterials and links17.2. - 21.2./24.2. - 28.2.Lab work introductionProjects overview Group work and projects application procedureBasic text processingSlovene text processing3.3. - 7.3.Text clusteringText classification\\xa0Language models, knowledge basesProjects sign up form\\xa0(deadline Friday midnight).Github classroom assignment\\xa0(deadline Friday midnight, only one group member creates a team, exactly three members for a group!). Use GROUP ID\\xa0from the projects sign up form as a team name.10.3. - 14.3.Neural networks introduction (TensorFlow, Keras)Word embeddings & visualizations (offensive language)RNNs vs. GRUs vs. LSTMs + examplesMultiple simple NN architectures example (Google Colab)Wednesday’s lab session at 5pm cancelled due to https://hackathon.si. You are invited to join!17.3. - 21.3.Introduction to PyTorchPyTorch LightningSLING tutorial (setup, Singularity, SLURM)First submission (Friday, 23:59)24.3. - 28.3.First submission defense (in person)No lab session on Wednesday 9am. Please attend other sessions.March 25: NVIDIA Day at UL FRI (more info)31.3. - 4.4.Transformers, BERT (custom task)BERT (tagging)\\xa0 KeyBERT (keyword extraction), TopicBERT (topic modeling)\\xa07.4. - 11.4.Generative and conversational AI14.4. - 18.4.Prompting and efficiently Fine-Tuning a Large Language ModelRetrieval Augmented Generation (RAG)ARNES & SDI hackathon\\xa0in Portorož. You can get up to additional 10%\\xa0for lab grade if you attend.21.4. - 25.4.(Mon. holiday)Graph neural networks for text processing28.4. - 2.5.(Thu., Fri. holiday)No lab sessionsSecond submission (Friday, 23:59)5.5. - 9.5.Second submission defense (in person)12.5. - 16.5.Consultations/Project work/Discussions(Please attend the lab sessions and discuss your work and ideas!)19.5. - 23.5.Project work/Online discussionsFinal submission deadline (Friday, 23:59)\\xa0 \\xa0 \\xa0 IMPORTANT: Put your repositories visibility to public before the deadline or shortly after!Peer review submission deadline (TBD)\\xa0 \\xa0 \\xa0 Peer review link (each group will get an email with repositories to review)!26.5. - 30.5.No organized lab sessions this weekFinal project presentations (TBD)Course obligationsPlease regularly check Weekly plan\\xa0and course announcements for possible changes. You are expected to attend the sessions but you must\\xa0attend the defense sessions. At the assignment defense dates at least one member of a group must be present, otherwise all need to provide their doctor’s justification. At the last assignment all members must be present and also need to understand all parts of the submitted solution.All the work must be submitted using your Github project repository. Submission deadlines are indicated in the table above. Submission defenses will be held during the lab sessions.Students must work in groups of three members! There can exist only one group of two members per project type. The distribution of work between members should be seen by commits within the repository.ObligationDescriptionFinal grade relevanceSubmission 1Project selection & simple corpus analysis\\xa0 - Group (three members) selection\\xa0 - Report containing Introduction, existing solutions/related work and initial ideas\\xa0 - Proposed project dataset/corpus/data/... \\xa0 - Well organized repositoryPassSubmission 2Initial implementation / baseline with results\\xa0 - Updated Submission 1 parts\\xa0 - Implemented at least one solution with analysis\\xa0 - Future directions and ideas\\xa0 - Well organized repositoryPassSubmission 3Final solution and report\\xa0 - Final report incl. analyses and discussions\\xa0 - Fully reproducible repository80%Peer reviewEvaluate your peer group\\'s work\\xa0 - Each group will check final submissions of two other peer groups having the same topic20%Total: 100%Grading criteriaAll the graded work is group work. All the work is graded following the scoring schema below. To successfully pass the laboratory work, you need to pass Submission 1 and 2, and gain 50% total from each of Submission 3 and Peer review. Use PUBLIC GROUP ID for public communication regarding your group. GROUP ID is your internal id of a group for which marks will be publicly available. ScoringScoring is done relative to achievements of all the participants in the course. Instructions will define the criteria the participants need to address and exactly fulfilling the instructions will result in score 8. All the other scores will be relative to the quality of the submitted work. The role of instructors is to find an appropriate clustering of all the works into 6 clusters. To better illustrate the scoring, schema will be as follows:10 - exceptional: Extraordinary results, quality of work or extremely well structured and justified report. There might still be some very minor possibilities for improvement.Repository is clear and runnable. Report is well organized, results are discussed, visualizations are added value to the text and well prepared. Apart from minimum criteria the group tried multiple of their novel ideas to approach the problem.9 - very good: Above average knowledge presentation with some errors. Same as above - it can be visible that a group had novel ideas but they got out of time or did not finish (polish) everything. The submission has multiple minor flaws.8 - good: Submission of solid work, mainly addressing given instructions only.Group implemented everything suggested by the minimum criteria but not investigated further (not found much related work, did not apply multiple other techniques, ...). 7 - superficial: Below average knowledge and submission of work with errors that show partial understanding.Group implemented everything suggested by the minimum criteria but did not discuss results well, performed simple analyses only, ... Also the report is not well organized, and lacks data for reproducibility.6 - sufficient: Minimum criteria addressed with some major errors and drawbacks.Group was trying to implement minimum criteria (or part only) but their work has many minor flaws or a few major ones. The report also reflects their motivation.5 - insufficient: \\xa0Too much lack of knowledge, too many major errors or no work-effort could be drawn from the submitted work.The group did not address one or more points of the minimum criteria and the report contains major flaws. It can be seen that the group did not invest enough time into the work.Final project preparation guidelines and peer review instructionsSome major remarks that you should keep in mind out are the following regarding final submission:Comment on all the specifics of your algorithms that you use or have designed (i.e. features, hyperparameters, ...). In some cases it is useful to include an image instead of providing long descriptions. When you include graphs/images, they must be readable and provide additional insight (e.g. if there are lots of lines across the image or little space between them, it is not okay). When you report on results, keep them as much as possible in one table, so that a reader can compare different configurations. Also, it is useful to bold the best results. Both images and tables should be self-contained - i.e., together with the caption they need to provide enough information to the reader to understand it\\'s meaning without reading text around.Keep your report concise and try to not submit a report longer than 4 pages + references + appendices. Also, make sure you follow the proposed template.Focus on reporting results and using sensible measures. Try to find examples where your algorithm works better and may not even work at all. Explain why and also justify the differences in approaches that you used. In case previous work exists for your dataset, put the best results of other researchers in your results table (even if your results are much lower).Your submitted work (repository and report) should be structured in a way that your colleagues would be able to understand and re-run everything. Include all dependencies for you projects:In case you have used a non-public (or semi-public) dataset, do not include it in the repository, just put your contact data or protected link to download data/provide other instructions to retrieve data.Datasets that are available elsewhere should be just linked in your report/repository. If you performed additional transformations on datasets, scripts for that should be available in the repository.Some of you used models that take longer to train. You can include those models (maybe just the best one) in the repository or elsewhere and link it. Lastly, check that your repositories are publicly available before the peer review period starts! IMPORTANT:Include links to all dependencies/corpora or include them in the repository, so that anyone can check your work. Also include annotated data (if you manually prepared a corpus) or trained models (if training takes a long time).For anyone that will review your work, it must be as simple as possible to run your code.Peer review instructions:Please find the projects you need to review (see link above).Each group needs to review projects of the same topic they have chosen. Submit your peer review scores in the Google Form (see link above).You will get a score also for your grading, depending on how much (of course by some margin) your grading will be different from the assistant\\'s grading. Follow the scoring criteria as presented above and include feedback to your mark.Final project presentation instructionsEach group will have max. 3 minutes (STRICT)\\xa0to present their project. I will put your report to the projector and you will present it along with your report. I propose that you focus on specific interesting part that you will present (e.g., table, graph, figure, ...). The most important aspect to present is:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0What is the \"take-away message\" of your work? This should be concrete and concise, so that anyone can understand (also a completely lay person).See timetable above for time slots of your presentation. If you cannot attend, please write to me to get an alternative time slot.Specific projects informationProject 1: Conversational Agent with Retrieval-Augmented Generation (Aleš): Develop a conversational agent that enhances the quality and accuracy of its responses by dynamically retrieving and integrating relevant external documents from the web. Unlike traditional chatbots that rely solely on pre-trained knowledge, this system will perform real-time information retrieval, ensuring up-to-date answers. Potential applications include customer support, academic research assistance, and general knowledge queries. The project will involve natural language processing (NLP), web scraping, and retrieval-augmented generation (RAG) techniques to optimize answer quality.Proposed methodology:Literature review: Survey current approaches to Retrieval-Augmented Generation (RAG) and its applications in conversational systems.Propose a relevant application and relevant datasets for training and evaluation.Implement a pipeline for real-time information retrieval from external sources (e.g., Wikipedia, news articles, or academic papers), develop a conversational agent that incorporates retrieved information into responses using pre-trained models, experiment with prompt engineering techniques to improve coherence and factual accuracy of generated responses.Evaluate response quality automatically using BLEU, ROUGE, and also human evaluation metrics such as coherence, informativeness, and factual consistency.Prepare a final report with insights on how retrieval-augmented conversational agents can enhance answer quality in real-world applications.References:Gao, Y. et al. (2024) Retrieval-augmented generation for large language models: A survey, arXiv.org. Available at: https://arxiv.org/abs/2312.10997\\xa0 (Accessed: 17 February 2025).Chen, J., Lin, H., Han, X., & Sun, L. (2024). Benchmarking Large Language Models in Retrieval-Augmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16), 17754-17762. https://doi.org/10.1609/aaai.v38i16.29728\\xa0Project 2: Multilingual classification and organization of instruction-tuning examples (Aleš): Students will develop a multilingual classifier to categorize instruction-tuning examples into pre-defined categories. The project involves training deep learning models on datasets such as DBpedia and other relevant sources with a goal to produce balanced instruction-following capabilities in AI systems and detect their weak areas (domains) as well. The classifier should effectively distinguish different instruction types and improve dataset organization to support instruction-tuned LLMs. Proposed methodology:Literature review: Conduct a survey of current approaches to text classification, focusing on instruction-tuning datasets and their categorization. Review different strategies for creation of instruction tuning datasets (see Čibej and Touron et al. references).Identify and acquire relevant datasets. In this step, the main objective is to find relevant datasets that will help to organize and find weaknesses of the Slovene instruction-following dataset (http://hdl.handle.net/11356/1971). \\xa0Students should compare the Slovene dataset with existing English datasets, and find which areas are less covered or missing in the Slovene version (type of instruction, domains, etc.). Fine-tune pre-trained models on the selected datasets, optimizing for multi-label classification. Generate new instruction-tuning samples for less-covered areas. Assess model performance using appropriate classification metrics. Evaluate manually generated instruction-tuned samples using appropriate metrics. Provide insights on improving instruction dataset classification for AI models and prepare a final project report.References:Zhang, Shengyu et al. (2024) Instruction tuning for large language models: A survey, arXiv.org. Available at: https://arxiv.org/abs/2308.10792\\xa0 (Accessed: 17 February 2025).Vreš, Domen; et al., 2024, Slovene instruction-following dataset for large language models GaMS-Instruct-GEN 1.0, Slovenian language resource repository CLARIN.SI, ISSN 2820-4042, http://hdl.handle.net/11356/1971.Čibej, J., 2024. First Steps Toward the Compilation of a Safety Dataset for Slovene Large Language Models. Conference on Language Technologies and Digital Humanities (JT-DH-2024), Ljubljana, Slovenia.\\xa0https://doi.org/10.5281/zenodo.13936388Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. https://arxiv.org/abs/2307.09288. Project 3: Improving prompt sensitivity of LLMs (Aleš): This project aims to experiment with techniques to reduce the sensitivity of large language models (LLMs) to variations in user prompts. Often, minor changes in phrasing can lead to significantly different responses, affecting consistency and reliability. Students will explore methods such as prompt paraphrasing, fine-tuning with slightly altered prompts to make LLM outputs more stable across semantically similar queries. The project will involve generating diverse paraphrases of prompts, analyzing model behavior, and fine-tuning responses to enhance reliability. This research is crucial for improving LLM usability in real-world applications, ensuring more consistent and predictable answers. Proposed methodology:Literature review: Review techniques used in prompt engineering, adversarial robustness, and controlled text generation.Select a dataset: Identify and curate datasets with diverse prompt variations, slightly altered prompts, etc. Implement and test various techniques for overcoming prompt sensitivity: paraphrase generation techniques, etc.Conduct human evaluation and measure response consistency using automated metrics.Prepare report and analyze results to determine which techniques most effectively reduce prompt sensitivity.References:Mizrahi, M. et al. (2024) State of what art? A call for multi-prompt LLM Evaluation, ACL Anthology. Available at: https://aclanthology.org/2024.tacl-1.52/\\xa0 \\xa0Cao, B. et al. (2024) On the worst prompt performance of large language models, arXiv.org. Available at: https://arxiv.org/abs/2406.10248\\xa0 Chatterjee, A. et al. (2024) POSIX: A prompt sensitivity index for large language models, ACL Anthology. Available at: https://aclanthology.org/2024.findings-emnlp.852/ (Accessed: 17 February 2025).Project 4: Answer Generation with Lexical Knowledge Retrieval (Also suitable for for students of digital linguistics, Slavko): The goal of this project is to enhance a Slovene large language model\\'s ability to answer lexicographic questions by utilizing structured information from Slovene lexicographic databases. The project will be implemented as a Retrieval-Augmented Generation (RAG) pipeline and will include an evaluation of the model\\'s performance with and without additional structured information.Proposed methodology:Literature Review: Conduct a comprehensive review of relevant academic literature on RAG models and their applications in lexicographic question-answering.Lexical Database Selection: Identify and select appropriate Slovene lexicographic databases that will serve as the source of structured knowledge.Definition of Lexical Tasks for Testing: Define a set of lexicographic tasks that will be used to evaluate the model’s performance. These tasks should reflect real-world lexical queries.Manual Creation of a Small Test Dataset: Construct a small but diverse test dataset to be used for initial evaluation of the model’s capabilities.Creation of the Knowledge Retrieval System: Develop a system for retrieving relevant information from the selected lexical databases to support the RAG pipeline.Integration of Structured Information into the Model: Implement the integration of retrieved structured information into the Slovene large language model.Evaluation of the RAG Pipeline: Assess the model’s performance using appropriate evaluation metrics, comparing results with and without structured lexical information.Recommended LiteratureFan, W., Ding, Y., Ning, L., Wang, S., Li, H., Yin, D., Chua, T.S., & Li, Q. (2024). \"A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models.\" Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 6491-6501.Chouhan, A., & Gertz, M. (2024). \"LexDrafter: Terminology Drafting for Legislative Documents Using Retrieval Augmented Generation.\" Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 10448-10458.Beguš G., Dąbkowski M., Rhodes R. (2023): \"Large Linguistic Models: Investigating LLMs\\' metalinguistic abilities.\" https://arxiv.org/abs/2305.00948\\xa0Project 5: Integrating Structured Knowledge into Large Language Models (Slavko): This project aims to explore advanced techniques for integrating structured knowledge in the form of knowledge graphs into large language models (LLMs). The objective is to evaluate different methods for incorporating knowledge graphs to improve the model\\'s ability to answer complex questions accurately.MethodologyLiterature Review: Conduct a thorough review of existing research on the integration of knowledge graphs into LLMs.Creation of an Evaluation Set: Construct a small evaluation dataset consisting of questions paired with corresponding knowledge graphs. These graphs will contain hints that should assist the model in generating correct answers.Selection of Integration Techniques: Identify and develop multiple techniques for injecting knowledge graphs into the model, such as:Direct input augmentation: Graph embedding-based integrationAttention-based fusion techniquesImplementation of Knowledge Graph InjectionImplement the selected techniques and integrate knowledge graphs into the LLM.Evaluation and Performance Analysis: Assess the effectiveness of each technique by measuring improvements in model performance, using appropriate evaluation metrics such as accuracy, precision, recall, and F1-score.Recommended LiteratureTang, Jiabin, et al. \"Graphgpt: Graph instruction tuning for large language models.\" Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2024.Kau, A., He, X., Nambissan, A., Astudillo, A., Yin, H., & Aryani, A. (2024). \"Combining Knowledge Graphs and Large Language Models.\" arXiv preprint arXiv:2407.06564.Yasunaga, Michihiro, et al. \"Deep bidirectional language-knowledge graph pretraining.\" Advances in Neural Information Processing Systems\\xa035 (2022), pp. 37309-37323.Plenz, Moritz, and Anette Frank. \"Graph Language Models.\" arXiv preprint arXiv:2401.07105\\xa0(2024).Project 6: Automatic generation of Slovenian traffic news for RTV Slovenija (Slavko): This project aims to use an existing LLM, »fine-tune« it, leverage prompt engineering techniques to generate short traffic reports. You are given Excel data from promet.si portal and your goal is to generate regular and important traffic news that are read by the radio presenters at RTV Slovenija. You also need to take into account guidelines and instructions to form the news. Currently, they hire students to manually check and type reports that are read every 30 minutes.MethodologyLiterature Review: Conduct a thorough review of existing research and select appropriate LLMs for the task. Review and prepare an exploratory report on the data provided.Initial solution: Try to solve the task initially only by using prompt engineering techniques.Evaulation definition: Define (semi-)automatic evaluation criteria and implement it. Take the following into account: identification of important news, correct roads namings, correct filtering, text lengths and words, ...LLM (Parameter-efficient) fine-tuning: Improve an existing LLM to perform the task automatically. Provide an interface to do an interactive test.Evaluation and Performance Analysis: Assess the effectiveness of each technique by measuring improvements in model performance, using appropriate automatic (P, R, F1) and human evaluation metrics.Recommended LiteratureLab session materialsRTV Slo data: LINK\\xa0(zip format). The data consists of:Promet.si input resources (Podatki - PrometnoPorocilo_2022_2023_2024.xlsx).RTV Slo news texts to be read through the radio stations (Podatki - rtvslo.si).Additional instructions for the students that manually type news texts (PROMET, osnove.docx, PROMET.docx).Project 7: Analysis and comparison of translation errors and biases in LLMs (For students of digital linguistics only, Aleš): This project focuses on analyzing and comparing translation errors and biases in large language models (LLMs). Students will evaluate how different models handle translations, identifying common errors such as mistranslations, omissions, and cultural misinterpretations. Additionally, the project will explore biases that emerge in translations, including gender, regional, and political biases. By systematically comparing multiple LLMs, students will assess translation quality using both automated metrics and human evaluations. The findings will help improve fairness and accuracy in AI-driven translation systems.Proposed methodology:Literature review: Conduct a survey of research on translation errors and biases in large language models (LLMs).Define a problem and select an appropriate dataset. Problems should be specific: Examine bias-related issues, such as gender bias in pronoun translation, political bias in news articles.Choose one open-source and one closed-source model. Experiment with various prompt techniques related to your project goal. Develop and perform qualitative analysis of translation errors and biases. Prepare a final report with detailed observations, qualitative analysis, and practical suggestions for future improvements. \\xa0References:Roberto Navigli, Simone Conia, and Björn Ross. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. J. Data and Information Quality 15, 2, Article 10 (June 2023). https://doi.org/10.1145/3597307Barclay, P. J., & Sami, A. (2024). Investigating Markers and Drivers of Gender Bias in Machine Translations. arXiv.Org, abs/2403.11896. https://doi.org/10.48550/arxiv.2403.11896')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb6c70e3bac9f3",
   "metadata": {},
   "source": [
    "We loaded the entire course into a single document. Since the sentence transformer can handle only limited sections of text, this might be problematic. Next, we will use the `RecursiveCharacterTextSplitter` to split the document into chunks with a `chunk_size` of 1000 characters and a `chunk_overlap` of 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e9d0474faae5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2124183bcfebb",
   "metadata": {},
   "source": [
    "Let's see what the chunked document looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a78d50ca11f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfa7828d59a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd072c543c6634e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b39987bd2a858",
   "metadata": {},
   "source": [
    "Next, we initialize a vector store. A vector store is a data structure that functions as a vector database, where each document is stored based on its own embedding. We will use the `FAISS` library for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4aef8084406765",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(all_splits, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639ebbad8683113",
   "metadata": {},
   "source": [
    "Finally, we define a retriever—an object that will handle the **retrieving** part of the RAG pipeline. The retriever receives as arguments the metric by which we search the space and the number of the k-nearest documents (chunks in our case) that we retrieve to present to the studen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e58a34f778c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44827c5c8dc5a5ab",
   "metadata": {},
   "source": [
    "Next, we modify our prompt template so that it now receives the context (the documents selected by the RAG system) and then generates the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6fe7081ceab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Reply your answer in markdown format.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE.strip(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aefe8951eaacc1",
   "metadata": {},
   "source": [
    "With the prompt defined, we next set up the `ConversationalRetrievalChain` that will utilize the defined `retriever` and `llm`, following the `PROMPT_TEMPLATE` to extract documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805381d21c53361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct complete LLM chain\n",
    "llm_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retreiver,\n",
    "    return_source_documents=False,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5229a0d7fcf4a",
   "metadata": {},
   "source": [
    "Finally, we create the `answer_question` function that will handle the chain invocation for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8390ab941f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: dict[str] = None) -> str:\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    response = llm_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "    answer = response[\"answer\"].split(\"### Answer:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd223d6a1c8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do I have to do for the peer review?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c6b213b988ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the first project about?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b55adc68b293e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do you have to do for a good grade?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7e72b41ce6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the project intended for digital linguistics about?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b275530b5888fe",
   "metadata": {},
   "source": [
    "With the introduction of RAG, the `Mistral` model was able to successfully answer questions about the NLP course.\n",
    "\n",
    "**Exercises:**\n",
    "* Implement a data loader script that can load documents from a folder.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04c4186181e74445835d74274dbebee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16793b1557dd42598acf5df018f029ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36042acd80ec4804a9396a69db64820c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcb6d7031f69424f922518e6203e1543",
      "placeholder": "​",
      "style": "IPY_MODEL_a01f72420fea410d9f3e66f4f436ee63",
      "value": " 3/3 [01:13&lt;00:00, 24.30s/it]"
     }
    },
    "62898c578fc54ca6996c1dc74be15ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3df8004294c41eabbd8954a7b8ba8fd",
      "placeholder": "​",
      "style": "IPY_MODEL_dc41cc0f49374d27955be33ece1d4532",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "76dac92078e74b12a8d38527a8369acb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c204a58d85934023940648c4d736a7d6",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04c4186181e74445835d74274dbebee9",
      "value": 3
     }
    },
    "a01f72420fea410d9f3e66f4f436ee63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3df8004294c41eabbd8954a7b8ba8fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c204a58d85934023940648c4d736a7d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc41cc0f49374d27955be33ece1d4532": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e33e7ceff6e942df9e6cea0270c0b2b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62898c578fc54ca6996c1dc74be15ad9",
       "IPY_MODEL_76dac92078e74b12a8d38527a8369acb",
       "IPY_MODEL_36042acd80ec4804a9396a69db64820c"
      ],
      "layout": "IPY_MODEL_16793b1557dd42598acf5df018f029ca"
     }
    },
    "fcb6d7031f69424f922518e6203e1543": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
