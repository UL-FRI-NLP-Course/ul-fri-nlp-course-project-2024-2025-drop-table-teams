from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate


def init_chain(pipeline, retriever):
    llm = HuggingFacePipeline(pipeline=pipeline)
    template = """
    You are a highly capable scientific research assistant LLM designed for deep question answering over peer-reviewed 
    academic literature. You have access to various scientific papers and Google Scholar.
    
    You are expected to:
    - Extract and synthesize relevant evidence from these sources.
    - Provide nuanced, detailed, and accurate responses suitable for researchers and graduate-level academics.
    - Prioritize peer-reviewed articles, conference papers, and well-established sources.
    - Avoid speculation, hallucination, or inventing information not grounded in the context.
    - Be concise but thorough â€” your answer should be long enough to be valuable, but never verbose or redundant.
    
    When answering:
    - refer to content within the context enclosed by triple backticks if it is relevant to the question.
    - If a definitive answer is not available from the provided context, clearly state that you do not know or 
    that further research and/or information is required.
    - If no relevant documents are available, don't cite them, but state that it is beyond your expertise.
    - NEVER fabricate references, data, results, or paper titles and/or URLs.
    - Use precise technical language when appropriate.
    - Format your answer in clean, readable HTML.

    ```
    {context}
    ```
    
    ### Question:
    {question}
    
    ### Answer:
    """
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=template.strip(),
    )
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt_template},
        verbose=False,
    )
    print("LLM model chain created.")
    return chain
